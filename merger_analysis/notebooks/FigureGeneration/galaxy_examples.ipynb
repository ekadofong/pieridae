{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Galaxy Examples - Classification Visualization\n",
    "\n",
    "This notebook creates a grid of galaxy examples from the BYOL merger analysis:\n",
    "- Layout: N rows (galaxies) √ó 3 columns (visualization types)\n",
    "- Visualizations: HSC r-N708-i RGB, HSC i-band (LSB), Starlet HF\n",
    "- Galaxy selection: merger candidates, undisturbed, fragmented examples\n",
    "\n",
    "## Configuration\n",
    "This notebook is config-driven and synced with `run_analysis.py`:\n",
    "- Main config (`../config.yaml`): BYOL data paths, label file\n",
    "- Figures config (`../configs/figures_config.yaml`): galaxy selection, visualization params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from astropy.visualization import make_lupton_rgb\n",
    "from astropy import coordinates\n",
    "from ekfplot import plot as ek\n",
    "\n",
    "# Add pieridae to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from carpenter import conventions, pixels\n",
    "from pieridae.starbursts import sample\n",
    "\n",
    "print(\"üì¶ Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str):\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# Load main config (BYOL analysis paths)\n",
    "main_config_path = Path.cwd().parent.parent / 'config.yaml'\n",
    "main_config = load_config(main_config_path)\n",
    "\n",
    "# Load figures config (galaxy selection and visualization)\n",
    "figures_config_path = Path.cwd().parent.parent / 'configs' / 'figures_config.yaml'\n",
    "figures_config = load_config(figures_config_path)\n",
    "\n",
    "# Convert paths to Path objects\n",
    "main_config['data']['input_path'] = Path(main_config['data']['input_path'])\n",
    "main_config['data']['output_path'] = Path(main_config['data']['output_path'])\n",
    "\n",
    "print(f\"üìã Main config loaded from: {main_config_path}\")\n",
    "print(f\"üìã Figures config loaded from: {figures_config_path}\")\n",
    "print(f\"üìÅ BYOL input path: {main_config['data']['input_path']}\")\n",
    "print(f\"üìÅ BYOL output path: {main_config['data']['output_path']}\")\n",
    "print(f\"üìÅ Figure output: {figures_config['figure_output']['output_dir']}\")\n",
    "print(f\"üéØ Selection mode: {figures_config['galaxy_selection']['mode']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog-header",
   "metadata": {},
   "source": [
    "## Load Catalog and Adjust Stellar Masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog\n",
    "catalog_file = figures_config['catalog']['catalog_file']\n",
    "print(f\"üìö Loading catalog from: {catalog_file}\")\n",
    "catalog, masks = sample.load_sample(catalog_file)\n",
    "print(f\"‚úÖ Loaded {len(catalog)} objects\")\n",
    "\n",
    "# Load adjusted stellar masses from BYOL analysis\n",
    "datadir = main_config['data']['input_path']\n",
    "print(f\"üìä Loading adjusted stellar masses from: {datadir}\")\n",
    "\n",
    "for sid in tqdm(catalog.index, desc=\"Loading masses\"):\n",
    "    filename = f'{datadir}/{sid}/{sid}_i_results.pkl'\n",
    "    if not os.path.exists(filename):\n",
    "        continue\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    catalog.loc[sid, 'logmass_adjusted'] = x['logmass_adjusted']\n",
    "\n",
    "# Fill missing values\n",
    "catalog.loc[catalog['logmass_adjusted'].isna(), 'logmass_adjusted'] = \\\n",
    "    catalog.loc[catalog['logmass_adjusted'].isna(), 'logmass']\n",
    "\n",
    "print(f\"‚úÖ Loaded adjusted masses for {(~catalog['logmass_adjusted'].isna()).sum()} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byol-header",
   "metadata": {},
   "source": [
    "## Load BYOL Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dimensionality reduction results from BYOL analysis\n",
    "results_path = main_config['data']['output_path'] / 'dimensionality_reduction_results.pkl'\n",
    "print(f\"üì• Loading BYOL analysis results from: {results_path}\")\n",
    "\n",
    "with open(results_path, 'rb') as f:\n",
    "    reduction_results = pickle.load(f)\n",
    "\n",
    "embeddings = reduction_results['embeddings_original']\n",
    "embeddings_pca = reduction_results['embeddings_pca']\n",
    "embeddings_umap = reduction_results['embeddings_umap']\n",
    "img_names = reduction_results['img_names']\n",
    "pca = reduction_results['pca']\n",
    "\n",
    "print(f\"‚úÖ Loaded embeddings for {len(img_names)} images\")\n",
    "print(f\"   PCA shape: {embeddings_pca.shape}\")\n",
    "print(f\"   UMAP shape: {embeddings_umap.shape}\")\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "images-header",
   "metadata": {},
   "source": [
    "## Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images for visualization\n",
    "import glob\n",
    "\n",
    "data_path = main_config['data']['input_path']\n",
    "pattern = f\"{data_path}/M*/*i_results.pkl\"\n",
    "filenames = glob.glob(pattern)\n",
    "\n",
    "print(f\"üîç Loading image data from: {data_path}\")\n",
    "print(f\"üì∏ Found {len(filenames)} image files\")\n",
    "\n",
    "imgs = []\n",
    "img_names_list = []\n",
    "\n",
    "for fname in tqdm(filenames, desc=\"Loading images\"):\n",
    "    img = []\n",
    "    for band in 'gi':\n",
    "        current_filename = fname.replace('_i_', f'_{band}_')\n",
    "        \n",
    "        try:\n",
    "            with open(current_filename, 'rb') as f:\n",
    "                xf = pickle.load(f)\n",
    "                img.append(xf['image'])\n",
    "                if band == 'i':\n",
    "                    img.append(xf['hf_image'])\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "    if len(img) == 3:  # Only add if we have all bands\n",
    "        imgs.append(np.array(img))\n",
    "        img_names_list.append(Path(fname).parent.name)\n",
    "\n",
    "images = np.array(imgs)\n",
    "img_names_array = np.array(img_names_list)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(images)} images with shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-header",
   "metadata": {},
   "source": [
    "## Load Classification Labels and Compute Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification labels\n",
    "label_file = Path(main_config['labels']['classifications_file'])\n",
    "print(f\"üìã Loading labels from: {label_file}\")\n",
    "\n",
    "mergers = pd.read_csv(label_file, index_col=0)\n",
    "labels = mergers.reindex(img_names).replace(np.nan, 0).values.flatten().astype(int)\n",
    "\n",
    "print(f\"‚úÖ Loaded classification labels: {len(labels)} objects\")\n",
    "\n",
    "# Print label distribution\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "label_meanings = main_config['labels']['label_mapping']\n",
    "\n",
    "print(\"üìä Label distribution:\")\n",
    "for label_val, count in zip(unique, counts):\n",
    "    meaning = label_meanings.get(label_val, f\"unknown_{label_val}\")\n",
    "    print(f\"   {label_val} ({meaning}): {count} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighbor-based probability labels\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Get parameters from config\n",
    "n_neighbors = figures_config['galaxy_selection']['n_neighbors']\n",
    "n_min = figures_config['galaxy_selection']['minimum_labeled_neighbors']\n",
    "\n",
    "print(f\"üîç Computing probabilities with {n_neighbors} neighbors\")\n",
    "print(f\"   Minimum labeled neighbors: {n_min}\")\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embeddings_pca)\n",
    "distances, indices = nbrs.kneighbors(embeddings_pca)\n",
    "distances[:, 0] = np.nan\n",
    "\n",
    "neighbor_labels = labels[indices]\n",
    "\n",
    "weights = np.where(neighbor_labels > 0, 1./distances, 0.)\n",
    "weights /= np.nansum(weights, axis=1).reshape(-1, 1)\n",
    "\n",
    "prob_labels = np.zeros([embeddings_pca.shape[0], labels.max()+1])\n",
    "\n",
    "for ix in range(labels.max()+1):\n",
    "    prob_labels[:, ix] = np.nansum(np.where(neighbor_labels==ix, weights, 0), axis=1)\n",
    "\n",
    "n_labels = np.sum(neighbor_labels > 0, axis=1)\n",
    "prob_labels[n_labels < n_min] = 0.\n",
    "\n",
    "print(f\"‚úÖ {(prob_labels>0).any(axis=1).sum()} galaxies have auto-labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iterative-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative label refinement\n",
    "n_min_auto = figures_config['galaxy_selection']['minimum_labeled_neighbors_for_autoprop']\n",
    "prob_threshold = main_config['labels']['prob_threshold']\n",
    "frag_threshold = main_config['labels']['frag_threshold']\n",
    "\n",
    "print(f\"üîÑ Iterative label refinement\")\n",
    "print(f\"   Min neighbors for auto-propagation: {n_min_auto}\")\n",
    "print(f\"   Probability threshold: {prob_threshold}\")\n",
    "print(f\"   Fragmentation threshold: {frag_threshold}\")\n",
    "\n",
    "iterative_labels = labels.copy()\n",
    "n_new = 1\n",
    "\n",
    "while n_new > 0:\n",
    "    neighbor_labels = iterative_labels[indices]\n",
    "    n_labels_iter = np.sum(neighbor_labels > 0, axis=1)\n",
    "    n_labeled = (iterative_labels > 0).sum()\n",
    "    \n",
    "    additions = np.where(prob_labels[n_labels_iter >= n_min_auto] > prob_threshold)\n",
    "    new_labels = np.zeros_like(iterative_labels)\n",
    "    new_labels[additions[0]] = additions[1]\n",
    "    \n",
    "    new_labels[(prob_labels[:, 4] > frag_threshold) & (n_labels_iter >= n_min_auto)] = 4\n",
    "    \n",
    "    iterative_labels[iterative_labels == 0] = new_labels[iterative_labels == 0]\n",
    "    is_iterative = labels != iterative_labels\n",
    "    n_new = (iterative_labels > 0).sum() - n_labeled\n",
    "    print(f\"   {(labels>0).sum()} human labels\")\n",
    "    print(f\"   {n_new} auto-labels added, {(iterative_labels>0).sum()} labels total\")\n",
    "    break\n",
    "\n",
    "# Recompute prob_labels with iterative labels\n",
    "neighbor_labels = iterative_labels[indices]\n",
    "\n",
    "weights = np.where(neighbor_labels > 0, 1./distances, 0.)\n",
    "weights[is_iterative] *= 0.1\n",
    "weights /= np.nansum(weights, axis=1).reshape(-1, 1)\n",
    "\n",
    "prob_labels = np.zeros([embeddings_pca.shape[0], labels.max()+1])\n",
    "\n",
    "for ix in range(labels.max()+1):\n",
    "    prob_labels[:, ix] = np.nansum(np.where(neighbor_labels==ix, weights, 0), axis=1)\n",
    "\n",
    "n_labels = np.sum(neighbor_labels > 0, axis=1)\n",
    "prob_labels[n_labels < n_min] = 0.\n",
    "\n",
    "print(f\"‚úÖ {(prob_labels>0).any(axis=1).sum()} galaxies have final auto-labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "## Select Example Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selection parameters from config\n",
    "mode = figures_config['galaxy_selection']['mode']\n",
    "mass_threshold = figures_config['galaxy_selection']['mass_threshold']\n",
    "prob_thresholds = figures_config['galaxy_selection']['prob_thresholds']\n",
    "random_seed = figures_config['galaxy_selection']['random_seed']\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "print(f\"üéØ Selecting galaxies in mode: {mode}\")\n",
    "print(f\"   Mass threshold: log(M*/Msun) < {mass_threshold}\")\n",
    "print(f\"   Probability thresholds: {prob_thresholds}\")\n",
    "\n",
    "# Define selection masks\n",
    "fragmented = prob_labels[:, 4] > prob_thresholds['fragmented']\n",
    "possible_merger = (prob_labels[:, 3] + prob_labels[:, 2]) > prob_labels[:, 1]\n",
    "low_mass = catalog.reindex(img_names)['logmass_adjusted'] < mass_threshold\n",
    "\n",
    "# Check for preselected galaxies\n",
    "preselected = figures_config['galaxy_selection'].get('preselected', {})\n",
    "if mode in preselected and preselected[mode] is not None:\n",
    "    all_selected = preselected[mode]\n",
    "    print(f\"‚úÖ Using preselected galaxies for {mode} mode: {all_selected}\")\n",
    "else:\n",
    "    # Random selection\n",
    "    selected_mergers = []\n",
    "    \n",
    "    # First merger candidate (possible merger)\n",
    "    merger_candidates = np.arange(prob_labels.shape[0])[possible_merger & ~fragmented & low_mass]\n",
    "    if len(merger_candidates) > 0:\n",
    "        selected_mergers.append(int(merger_candidates[np.random.randint(0, len(merger_candidates))]))\n",
    "    \n",
    "    # Second merger candidate (higher confidence)\n",
    "    merger_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 3] > prob_thresholds['merger']) & ~fragmented & low_mass]\n",
    "    if len(merger_candidates) > 0:\n",
    "        selected_mergers.append(int(merger_candidates[np.random.randint(0, len(merger_candidates))]))\n",
    "    \n",
    "    # Undisturbed example\n",
    "    undisturbed_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 1] > prob_thresholds['undisturbed']) & low_mass]\n",
    "    if len(undisturbed_candidates) > 0:\n",
    "        selected_undisturbed = int(undisturbed_candidates[np.random.randint(0, len(undisturbed_candidates))])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No undisturbed candidates found\")\n",
    "        selected_undisturbed = None\n",
    "    \n",
    "    # Fragmented example\n",
    "    fragmented_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 4] > prob_thresholds['fragmented']) & low_mass]\n",
    "    if len(fragmented_candidates) > 0:\n",
    "        selected_fragmented = int(fragmented_candidates[np.random.randint(0, len(fragmented_candidates))])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No fragmented candidates found\")\n",
    "        selected_fragmented = None\n",
    "    \n",
    "    # Combine all selected galaxies\n",
    "    all_selected = selected_mergers.copy()\n",
    "    if selected_undisturbed is not None:\n",
    "        all_selected.append(selected_undisturbed)\n",
    "    if selected_fragmented is not None:\n",
    "        all_selected.append(selected_fragmented)\n",
    "    \n",
    "    print(f\"‚úÖ Randomly selected {len(all_selected)} galaxies: {all_selected}\")\n",
    "\n",
    "# Get selected names\n",
    "selected_names = img_names[all_selected]\n",
    "print(f\"   Galaxy IDs: {' '.join(selected_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutouts-header",
   "metadata": {},
   "source": [
    "## Load Cutouts for Selected Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutouts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cutout data\n",
    "cutout_base = Path(figures_config['cutout_data']['local_path'])\n",
    "print(f\"üìÇ Loading cutouts from: {cutout_base}\")\n",
    "\n",
    "bbmb_d = {}\n",
    "\n",
    "for targetid, gid in zip(selected_names, all_selected):\n",
    "    objname = conventions.produce_merianobjectname(*catalog.loc[targetid, ['RA', 'DEC']].values)\n",
    "    bbmb = pixels.BBMBImage()\n",
    "\n",
    "    for band in ['r', 'N708', 'i']:\n",
    "        if band in ['N708', 'N540']:\n",
    "            cutout = f'{cutout_base}/merian/{objname}_{band}_merim.fits'\n",
    "        else:\n",
    "            cutout = f'{cutout_base}/hsc/{objname}_HSC-{band}.fits'\n",
    "        \n",
    "        if not os.path.exists(cutout): \n",
    "            bbmb = None\n",
    "            print(f'‚ö†Ô∏è  Skipping {targetid}, cutout not found: {cutout}')\n",
    "            break\n",
    "            \n",
    "        psf = None\n",
    "        bbmb.add_band(\n",
    "            band,\n",
    "            coordinates.SkyCoord(catalog.loc[targetid, 'RA'], catalog.loc[targetid, 'DEC'], unit='deg'),\n",
    "            size=150,\n",
    "            image=cutout,\n",
    "            var=cutout,\n",
    "            image_ext=1,\n",
    "            var_ext=3,\n",
    "        )    \n",
    "    bbmb_d[gid] = bbmb\n",
    "\n",
    "print(f\"‚úÖ Loaded cutouts for {len(bbmb_d)} galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Create Galaxy Examples Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get visualization parameters from config\n",
    "viz_config = figures_config['visualization']\n",
    "fig_config = figures_config['figure_output']\n",
    "\n",
    "n_galaxies = len(all_selected)\n",
    "n_viz_types = 3\n",
    "\n",
    "# Figure dimensions\n",
    "fig_width = fig_config['figsize']['width']\n",
    "fig_height = fig_config['figsize']['height_per_galaxy'] * n_galaxies\n",
    "\n",
    "print(f\"üé® Creating {n_galaxies}√ó{n_viz_types} visualization grid\")\n",
    "\n",
    "fig, axarr = plt.subplots(n_galaxies, n_viz_types, figsize=(fig_width, fig_height))\n",
    "\n",
    "# Handle case where we only have one galaxy (axarr would be 1D)\n",
    "if n_galaxies == 1:\n",
    "    axarr = axarr.reshape(1, -1)\n",
    "\n",
    "for row_idx, gix in enumerate(all_selected):\n",
    "    # Column 0: r-N708-i RGB image\n",
    "    bbmb = bbmb_d[gix]\n",
    "    if bbmb is None:\n",
    "        ek.imshow(\n",
    "            images[gix][1],\n",
    "            origin='lower',\n",
    "            cmap='Greys',\n",
    "            q=0.01,\n",
    "            ax=axarr[row_idx, 0]\n",
    "        )\n",
    "    else:\n",
    "        ek.imshow(\n",
    "            make_lupton_rgb(\n",
    "                bbmb.image['r'], \n",
    "                bbmb.image['N708'], \n",
    "                bbmb.image['i'], \n",
    "                Q=viz_config['lupton']['Q'], \n",
    "                stretch=viz_config['lupton']['stretch']\n",
    "            ),\n",
    "            ax=axarr[row_idx, 0]\n",
    "        )\n",
    "    \n",
    "    # Column 1: HSC i-band (LSB with SymLog normalization)\n",
    "    axarr[row_idx, 1].imshow(\n",
    "        images[gix][1],\n",
    "        origin='lower',\n",
    "        cmap=viz_config['lsb']['colormap'],\n",
    "        norm=colors.SymLogNorm(linthresh=viz_config['lsb']['linthresh'])\n",
    "    )\n",
    "    \n",
    "    # Column 2: Starlet HF\n",
    "    ek.imshow(\n",
    "        images[gix][2],\n",
    "        ax=axarr[row_idx, 2],\n",
    "        cmap=viz_config['hf']['colormap'],\n",
    "        q=viz_config['hf']['q']\n",
    "    )\n",
    "    \n",
    "    # Add probability labels to first column\n",
    "    label_config = viz_config['labels']\n",
    "    ek.text(\n",
    "        0.025,\n",
    "        0.025,\n",
    "        rf'''N$_{{{\\rm labels}}}$ = {n_labels[gix]}\n",
    "Pr[ud] = {prob_labels[gix, 1]:.2f}\n",
    "Pr[amb] = {prob_labels[gix, 2]:.2f}\n",
    "Pr[merg] = {prob_labels[gix, 3]:.2f}\n",
    "Pr[frag] = {prob_labels[gix, 4]:.2f}''',\n",
    "        ax=axarr[row_idx, 0],\n",
    "        fontsize=label_config['fontsize'],\n",
    "        bordercolor=label_config['bordercolor'],\n",
    "        color=label_config['textcolor'],\n",
    "        borderwidth=label_config['borderwidth']\n",
    "    )\n",
    "\n",
    "    # Add stellar mass to second column\n",
    "    with open(f\"{main_config['data']['input_path']}/{img_names[gix]}/{img_names[gix]}_i_results.pkl\", 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        logmstar = x['logmass_adjusted']\n",
    "\n",
    "    ek.text(\n",
    "        0.025,\n",
    "        0.025,\n",
    "        rf'''$\\log_{{10}}(\\frac{{M_{{\\bigstar}}}}{{M_\\odot}})={logmstar:.2f}$''',\n",
    "        ax=axarr[row_idx, 1],\n",
    "        fontsize=label_config['fontsize'],\n",
    "        bordercolor=label_config['bordercolor'],\n",
    "        color=label_config['textcolor'],\n",
    "        borderwidth=label_config['borderwidth']\n",
    "    )\n",
    "\n",
    "# Add row labels to first column\n",
    "row_labels = viz_config['row_labels']\n",
    "label_order = ['merger', 'ambiguous', 'undisturbed', 'fragmented']\n",
    "for row_idx, label_key in enumerate(label_order[:n_galaxies]):\n",
    "    if label_key in row_labels:\n",
    "        ek.text(\n",
    "            0.05, 0.95, \n",
    "            row_labels[label_key], \n",
    "            ax=axarr[row_idx, 0], \n",
    "            fontsize=label_config['fontsize_title'], \n",
    "            bordercolor='k', \n",
    "            color='w', \n",
    "            borderwidth=6\n",
    "        )\n",
    "\n",
    "# Add column headers to top row\n",
    "headers = viz_config['headers']\n",
    "if headers['col1']:\n",
    "    ek.text(\n",
    "        0.05, 0.95, \n",
    "        headers['col1'], \n",
    "        ax=axarr[0, 1], \n",
    "        fontsize=label_config['fontsize_header'], \n",
    "        bordercolor='k', \n",
    "        color='w', \n",
    "        borderwidth=6\n",
    "    )\n",
    "if headers['col2']:\n",
    "    ek.text(\n",
    "        0.05, 0.95, \n",
    "        headers['col2'], \n",
    "        ax=axarr[0, 2], \n",
    "        fontsize=label_config['fontsize_header'], \n",
    "        bordercolor='k', \n",
    "        color='w', \n",
    "        borderwidth=6\n",
    "    )\n",
    "\n",
    "# Remove ticks from all axes\n",
    "for ax in axarr.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=fig_config['wspace'], hspace=fig_config['hspace'])\n",
    "\n",
    "# Save figure\n",
    "output_dir = Path.cwd().parent.parent / fig_config['output_dir']\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "filename = fig_config['filename_pattern'].format(mode=mode)\n",
    "output_path = output_dir / filename\n",
    "\n",
    "plt.savefig(output_path, dpi=fig_config['dpi'], bbox_inches='tight')\n",
    "print(f\"‚úÖ Figure saved to: {output_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "end",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
