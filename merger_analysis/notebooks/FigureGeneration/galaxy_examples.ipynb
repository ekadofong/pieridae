{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Galaxy Examples - Classification Visualization\n",
    "\n",
    "This notebook creates a grid of galaxy examples from the BYOL merger analysis:\n",
    "- Layout: N rows (galaxies) × 3 columns (visualization types)\n",
    "- Visualizations: HSC r-N708-i RGB, HSC i-band (LSB), Starlet HF\n",
    "- Galaxy selection: merger candidates, undisturbed, fragmented examples\n",
    "\n",
    "## Configuration\n",
    "This notebook is config-driven and synced with `run_analysis.py`:\n",
    "- Main config (`../config.yaml`): BYOL data paths, label file\n",
    "- Figures config (`../configs/figures_config.yaml`): galaxy selection, visualization params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from astropy.visualization import make_lupton_rgb\n",
    "from astropy import coordinates\n",
    "from ekfplot import plot as ek\n",
    "\n",
    "# Add pieridae to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from carpenter import conventions, pixels\n",
    "from pieridae.starbursts import sample\n",
    "\n",
    "print(\"📦 Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str):\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# Load main config (BYOL analysis paths)\n",
    "main_config_path = Path.cwd().parent.parent / 'config.yaml'\n",
    "main_config = load_config(main_config_path)\n",
    "\n",
    "# Load figures config (galaxy selection and visualization)\n",
    "figures_config_path = Path.cwd().parent.parent / 'configs' / 'figures_config.yaml'\n",
    "figures_config = load_config(figures_config_path)\n",
    "\n",
    "# Convert paths to Path objects\n",
    "main_config['data']['input_path'] = Path(main_config['data']['input_path'])\n",
    "main_config['data']['output_path'] = Path(main_config['data']['output_path'])\n",
    "\n",
    "print(f\"📋 Main config loaded from: {main_config_path}\")\n",
    "print(f\"📋 Figures config loaded from: {figures_config_path}\")\n",
    "print(f\"📁 BYOL input path: {main_config['data']['input_path']}\")\n",
    "print(f\"📁 BYOL output path: {main_config['data']['output_path']}\")\n",
    "print(f\"📁 Figure output: {figures_config['figure_output']['output_dir']}\")\n",
    "print(f\"🎯 Selection mode: {figures_config['galaxy_selection']['mode']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog-header",
   "metadata": {},
   "source": [
    "## Load Catalog and Adjust Stellar Masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog\n",
    "catalog_file = figures_config['catalog']['catalog_file']\n",
    "print(f\"📚 Loading catalog from: {catalog_file}\")\n",
    "catalog, masks = sample.load_sample(catalog_file)\n",
    "print(f\"✅ Loaded {len(catalog)} objects\")\n",
    "\n",
    "# Load adjusted stellar masses from BYOL analysis\n",
    "datadir = main_config['data']['input_path']\n",
    "print(f\"📊 Loading adjusted stellar masses from: {datadir}\")\n",
    "\n",
    "for sid in tqdm(catalog.index, desc=\"Loading masses\"):\n",
    "    filename = f'{datadir}/{sid}/{sid}_i_results.pkl'\n",
    "    if not os.path.exists(filename):\n",
    "        continue\n",
    "    with open(filename, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    catalog.loc[sid, 'logmass_adjusted'] = x['logmass_adjusted']\n",
    "\n",
    "# Fill missing values\n",
    "catalog.loc[catalog['logmass_adjusted'].isna(), 'logmass_adjusted'] = \\\n",
    "    catalog.loc[catalog['logmass_adjusted'].isna(), 'logmass']\n",
    "\n",
    "print(f\"✅ Loaded adjusted masses for {(~catalog['logmass_adjusted'].isna()).sum()} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byol-header",
   "metadata": {},
   "source": [
    "## Load BYOL Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dimensionality reduction results from BYOL analysis\n",
    "results_path = main_config['data']['output_path'] / 'dimensionality_reduction_results.pkl'\n",
    "print(f\"📥 Loading BYOL analysis results from: {results_path}\")\n",
    "\n",
    "with open(results_path, 'rb') as f:\n",
    "    reduction_results = pickle.load(f)\n",
    "\n",
    "embeddings = reduction_results['embeddings_original']\n",
    "embeddings_pca = reduction_results['embeddings_pca']\n",
    "embeddings_umap = reduction_results['embeddings_umap']\n",
    "img_names = reduction_results['img_names']\n",
    "pca = reduction_results['pca']\n",
    "\n",
    "print(f\"✅ Loaded embeddings for {len(img_names)} images\")\n",
    "print(f\"   PCA shape: {embeddings_pca.shape}\")\n",
    "print(f\"   UMAP shape: {embeddings_umap.shape}\")\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "images-header",
   "metadata": {},
   "source": [
    "## Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images for visualization\n",
    "import glob\n",
    "\n",
    "data_path = main_config['data']['input_path']\n",
    "pattern = f\"{data_path}/M*/*i_results.pkl\"\n",
    "filenames = glob.glob(pattern)\n",
    "\n",
    "print(f\"🔍 Loading image data from: {data_path}\")\n",
    "print(f\"📸 Found {len(filenames)} image files\")\n",
    "\n",
    "imgs = []\n",
    "img_names_list = []\n",
    "\n",
    "for fname in tqdm(filenames, desc=\"Loading images\"):\n",
    "    img = []\n",
    "    for band in 'gi':\n",
    "        current_filename = fname.replace('_i_', f'_{band}_')\n",
    "        \n",
    "        try:\n",
    "            with open(current_filename, 'rb') as f:\n",
    "                xf = pickle.load(f)\n",
    "                img.append(xf['image'])\n",
    "                if band == 'i':\n",
    "                    img.append(xf['hf_image'])\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "    if len(img) == 3:  # Only add if we have all bands\n",
    "        imgs.append(np.array(img))\n",
    "        img_names_list.append(Path(fname).parent.name)\n",
    "\n",
    "images = np.array(imgs)\n",
    "img_names_array = np.array(img_names_list)\n",
    "\n",
    "print(f\"✅ Loaded {len(images)} images with shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-header",
   "metadata": {},
   "source": [
    "## Load Classification Labels and Compute Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification labels\n",
    "label_file = Path(main_config['labels']['classifications_file'])\n",
    "print(f\"📋 Loading labels from: {label_file}\")\n",
    "\n",
    "mergers = pd.read_csv(label_file, index_col=0)\n",
    "labels = mergers.reindex(img_names).replace(np.nan, 0).values.flatten().astype(int)\n",
    "\n",
    "print(f\"✅ Loaded classification labels: {len(labels)} objects\")\n",
    "\n",
    "# Print label distribution\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "label_meanings = main_config['labels']['label_mapping']\n",
    "\n",
    "print(\"📊 Label distribution:\")\n",
    "for label_val, count in zip(unique, counts):\n",
    "    meaning = label_meanings.get(label_val, f\"unknown_{label_val}\")\n",
    "    print(f\"   {label_val} ({meaning}): {count} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighbor-based probability labels\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Get parameters from config\n",
    "n_neighbors = figures_config['galaxy_selection']['n_neighbors']\n",
    "n_min = figures_config['galaxy_selection']['minimum_labeled_neighbors']\n",
    "\n",
    "print(f\"🔍 Computing probabilities with {n_neighbors} neighbors\")\n",
    "print(f\"   Minimum labeled neighbors: {n_min}\")\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embeddings_pca)\n",
    "distances, indices = nbrs.kneighbors(embeddings_pca)\n",
    "distances[:, 0] = np.nan\n",
    "\n",
    "neighbor_labels = labels[indices]\n",
    "\n",
    "weights = np.where(neighbor_labels > 0, 1./distances, 0.)\n",
    "weights /= np.nansum(weights, axis=1).reshape(-1, 1)\n",
    "\n",
    "prob_labels = np.zeros([embeddings_pca.shape[0], labels.max()+1])\n",
    "\n",
    "for ix in range(labels.max()+1):\n",
    "    prob_labels[:, ix] = np.nansum(np.where(neighbor_labels==ix, weights, 0), axis=1)\n",
    "\n",
    "n_labels = np.sum(neighbor_labels > 0, axis=1)\n",
    "prob_labels[n_labels < n_min] = 0.\n",
    "\n",
    "print(f\"✅ {(prob_labels>0).any(axis=1).sum()} galaxies have auto-labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iterative-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative label refinement\n",
    "n_min_auto = figures_config['galaxy_selection']['minimum_labeled_neighbors_for_autoprop']\n",
    "prob_threshold = main_config['labels']['prob_threshold']\n",
    "frag_threshold = main_config['labels']['frag_threshold']\n",
    "\n",
    "print(f\"🔄 Iterative label refinement\")\n",
    "print(f\"   Min neighbors for auto-propagation: {n_min_auto}\")\n",
    "print(f\"   Probability threshold: {prob_threshold}\")\n",
    "print(f\"   Fragmentation threshold: {frag_threshold}\")\n",
    "\n",
    "iterative_labels = labels.copy()\n",
    "n_new = 1\n",
    "\n",
    "while n_new > 0:\n",
    "    neighbor_labels = iterative_labels[indices]\n",
    "    n_labels_iter = np.sum(neighbor_labels > 0, axis=1)\n",
    "    n_labeled = (iterative_labels > 0).sum()\n",
    "    \n",
    "    additions = np.where(prob_labels[n_labels_iter >= n_min_auto] > prob_threshold)\n",
    "    new_labels = np.zeros_like(iterative_labels)\n",
    "    new_labels[additions[0]] = additions[1]\n",
    "    \n",
    "    new_labels[(prob_labels[:, 4] > frag_threshold) & (n_labels_iter >= n_min_auto)] = 4\n",
    "    \n",
    "    iterative_labels[iterative_labels == 0] = new_labels[iterative_labels == 0]\n",
    "    is_iterative = labels != iterative_labels\n",
    "    n_new = (iterative_labels > 0).sum() - n_labeled\n",
    "    print(f\"   {(labels>0).sum()} human labels\")\n",
    "    print(f\"   {n_new} auto-labels added, {(iterative_labels>0).sum()} labels total\")\n",
    "    break\n",
    "\n",
    "# Recompute prob_labels with iterative labels\n",
    "neighbor_labels = iterative_labels[indices]\n",
    "\n",
    "weights = np.where(neighbor_labels > 0, 1./distances, 0.)\n",
    "weights[is_iterative] *= 0.1\n",
    "weights /= np.nansum(weights, axis=1).reshape(-1, 1)\n",
    "\n",
    "prob_labels = np.zeros([embeddings_pca.shape[0], labels.max()+1])\n",
    "\n",
    "for ix in range(labels.max()+1):\n",
    "    prob_labels[:, ix] = np.nansum(np.where(neighbor_labels==ix, weights, 0), axis=1)\n",
    "\n",
    "n_labels = np.sum(neighbor_labels > 0, axis=1)\n",
    "prob_labels[n_labels < n_min] = 0.\n",
    "\n",
    "print(f\"✅ {(prob_labels>0).any(axis=1).sum()} galaxies have final auto-labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "## Select Example Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selection parameters from config\n",
    "mode = figures_config['galaxy_selection']['mode']\n",
    "mass_threshold = figures_config['galaxy_selection']['mass_threshold']\n",
    "prob_thresholds = figures_config['galaxy_selection']['prob_thresholds']\n",
    "random_seed = figures_config['galaxy_selection']['random_seed']\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "print(f\"🎯 Selecting galaxies in mode: {mode}\")\n",
    "print(f\"   Mass threshold: log(M*/Msun) < {mass_threshold}\")\n",
    "print(f\"   Probability thresholds: {prob_thresholds}\")\n",
    "\n",
    "# Define selection masks\n",
    "fragmented = prob_labels[:, 4] > prob_thresholds['fragmented']\n",
    "possible_merger = (prob_labels[:, 3] + prob_labels[:, 2]) > prob_labels[:, 1]\n",
    "low_mass = catalog.reindex(img_names)['logmass_adjusted'] < mass_threshold\n",
    "\n",
    "# Check for preselected galaxies\n",
    "preselected = figures_config['galaxy_selection'].get('preselected', {})\n",
    "if mode in preselected and preselected[mode] is not None:\n",
    "    all_selected = preselected[mode]\n",
    "    print(f\"✅ Using preselected galaxies for {mode} mode: {all_selected}\")\n",
    "else:\n",
    "    # Random selection\n",
    "    selected_mergers = []\n",
    "    \n",
    "    # First merger candidate (possible merger)\n",
    "    merger_candidates = np.arange(prob_labels.shape[0])[possible_merger & ~fragmented & low_mass]\n",
    "    if len(merger_candidates) > 0:\n",
    "        selected_mergers.append(int(merger_candidates[np.random.randint(0, len(merger_candidates))]))\n",
    "    \n",
    "    # Second merger candidate (higher confidence)\n",
    "    merger_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 3] > prob_thresholds['merger']) & ~fragmented & low_mass]\n",
    "    if len(merger_candidates) > 0:\n",
    "        selected_mergers.append(int(merger_candidates[np.random.randint(0, len(merger_candidates))]))\n",
    "    \n",
    "    # Undisturbed example\n",
    "    undisturbed_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 1] > prob_thresholds['undisturbed']) & low_mass]\n",
    "    if len(undisturbed_candidates) > 0:\n",
    "        selected_undisturbed = int(undisturbed_candidates[np.random.randint(0, len(undisturbed_candidates))])\n",
    "    else:\n",
    "        print(\"⚠️  No undisturbed candidates found\")\n",
    "        selected_undisturbed = None\n",
    "    \n",
    "    # Fragmented example\n",
    "    fragmented_candidates = np.arange(prob_labels.shape[0])[(prob_labels[:, 4] > prob_thresholds['fragmented']) & low_mass]\n",
    "    if len(fragmented_candidates) > 0:\n",
    "        selected_fragmented = int(fragmented_candidates[np.random.randint(0, len(fragmented_candidates))])\n",
    "    else:\n",
    "        print(\"⚠️  No fragmented candidates found\")\n",
    "        selected_fragmented = None\n",
    "    \n",
    "    # Combine all selected galaxies\n",
    "    all_selected = selected_mergers.copy()\n",
    "    if selected_undisturbed is not None:\n",
    "        all_selected.append(selected_undisturbed)\n",
    "    if selected_fragmented is not None:\n",
    "        all_selected.append(selected_fragmented)\n",
    "    \n",
    "    print(f\"✅ Randomly selected {len(all_selected)} galaxies: {all_selected}\")\n",
    "\n",
    "# Get selected names\n",
    "selected_names = img_names[all_selected]\n",
    "print(f\"   Galaxy IDs: {' '.join(selected_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutouts-header",
   "metadata": {},
   "source": [
    "## Load Cutouts for Selected Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutouts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cutout data\n",
    "cutout_base = Path(figures_config['cutout_data']['local_path'])\n",
    "print(f\"📂 Loading cutouts from: {cutout_base}\")\n",
    "\n",
    "bbmb_d = {}\n",
    "\n",
    "for targetid, gid in zip(selected_names, all_selected):\n",
    "    objname = conventions.produce_merianobjectname(*catalog.loc[targetid, ['RA', 'DEC']].values)\n",
    "    bbmb = pixels.BBMBImage()\n",
    "\n",
    "    for band in ['r', 'N708', 'i']:\n",
    "        if band in ['N708', 'N540']:\n",
    "            cutout = f'{cutout_base}/merian/{objname}_{band}_merim.fits'\n",
    "        else:\n",
    "            cutout = f'{cutout_base}/hsc/{objname}_HSC-{band}.fits'\n",
    "        \n",
    "        if not os.path.exists(cutout): \n",
    "            bbmb = None\n",
    "            print(f'⚠️  Skipping {targetid}, cutout not found: {cutout}')\n",
    "            break\n",
    "            \n",
    "        psf = None\n",
    "        bbmb.add_band(\n",
    "            band,\n",
    "            coordinates.SkyCoord(catalog.loc[targetid, 'RA'], catalog.loc[targetid, 'DEC'], unit='deg'),\n",
    "            size=150,\n",
    "            image=cutout,\n",
    "            var=cutout,\n",
    "            image_ext=1,\n",
    "            var_ext=3,\n",
    "        )    \n",
    "    bbmb_d[gid] = bbmb\n",
    "\n",
    "print(f\"✅ Loaded cutouts for {len(bbmb_d)} galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Create Galaxy Examples Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get visualization parameters from config\n",
    "viz_config = figures_config['visualization']\n",
    "fig_config = figures_config['figure_output']\n",
    "\n",
    "n_galaxies = len(all_selected)\n",
    "n_viz_types = 3\n",
    "\n",
    "# Figure dimensions\n",
    "fig_width = fig_config['figsize']['width']\n",
    "fig_height = fig_config['figsize']['height_per_galaxy'] * n_galaxies\n",
    "\n",
    "print(f\"🎨 Creating {n_galaxies}×{n_viz_types} visualization grid\")\n",
    "\n",
    "fig, axarr = plt.subplots(n_galaxies, n_viz_types, figsize=(fig_width, fig_height))\n",
    "\n",
    "# Handle case where we only have one galaxy (axarr would be 1D)\n",
    "if n_galaxies == 1:\n",
    "    axarr = axarr.reshape(1, -1)\n",
    "\n",
    "for row_idx, gix in enumerate(all_selected):\n",
    "    # Column 0: r-N708-i RGB image\n",
    "    bbmb = bbmb_d[gix]\n",
    "    if bbmb is None:\n",
    "        ek.imshow(\n",
    "            images[gix][1],\n",
    "            origin='lower',\n",
    "            cmap='Greys',\n",
    "            q=0.01,\n",
    "            ax=axarr[row_idx, 0]\n",
    "        )\n",
    "    else:\n",
    "        ek.imshow(\n",
    "            make_lupton_rgb(\n",
    "                bbmb.image['r'], \n",
    "                bbmb.image['N708'], \n",
    "                bbmb.image['i'], \n",
    "                Q=viz_config['lupton']['Q'], \n",
    "                stretch=viz_config['lupton']['stretch']\n",
    "            ),\n",
    "            ax=axarr[row_idx, 0]\n",
    "        )\n",
    "    \n",
    "    # Column 1: HSC i-band (LSB with SymLog normalization)\n",
    "    axarr[row_idx, 1].imshow(\n",
    "        images[gix][1],\n",
    "        origin='lower',\n",
    "        cmap=viz_config['lsb']['colormap'],\n",
    "        norm=colors.SymLogNorm(linthresh=viz_config['lsb']['linthresh'])\n",
    "    )\n",
    "    \n",
    "    # Column 2: Starlet HF\n",
    "    ek.imshow(\n",
    "        images[gix][2],\n",
    "        ax=axarr[row_idx, 2],\n",
    "        cmap=viz_config['hf']['colormap'],\n",
    "        q=viz_config['hf']['q']\n",
    "    )\n",
    "    \n",
    "    # Add probability labels to first column\n",
    "    label_config = viz_config['labels']\n",
    "    ek.text(\n",
    "        0.025,\n",
    "        0.025,\n",
    "        rf'''N$_{{{\\rm labels}}}$ = {n_labels[gix]}\n",
    "Pr[ud] = {prob_labels[gix, 1]:.2f}\n",
    "Pr[amb] = {prob_labels[gix, 2]:.2f}\n",
    "Pr[merg] = {prob_labels[gix, 3]:.2f}\n",
    "Pr[frag] = {prob_labels[gix, 4]:.2f}''',\n",
    "        ax=axarr[row_idx, 0],\n",
    "        fontsize=label_config['fontsize'],\n",
    "        bordercolor=label_config['bordercolor'],\n",
    "        color=label_config['textcolor'],\n",
    "        borderwidth=label_config['borderwidth']\n",
    "    )\n",
    "\n",
    "    # Add stellar mass to second column\n",
    "    with open(f\"{main_config['data']['input_path']}/{img_names[gix]}/{img_names[gix]}_i_results.pkl\", 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        logmstar = x['logmass_adjusted']\n",
    "\n",
    "    ek.text(\n",
    "        0.025,\n",
    "        0.025,\n",
    "        rf'''$\\log_{{10}}(\\frac{{M_{{\\bigstar}}}}{{M_\\odot}})={logmstar:.2f}$''',\n",
    "        ax=axarr[row_idx, 1],\n",
    "        fontsize=label_config['fontsize'],\n",
    "        bordercolor=label_config['bordercolor'],\n",
    "        color=label_config['textcolor'],\n",
    "        borderwidth=label_config['borderwidth']\n",
    "    )\n",
    "\n",
    "# Add row labels to first column\n",
    "row_labels = viz_config['row_labels']\n",
    "label_order = ['merger', 'ambiguous', 'undisturbed', 'fragmented']\n",
    "for row_idx, label_key in enumerate(label_order[:n_galaxies]):\n",
    "    if label_key in row_labels:\n",
    "        ek.text(\n",
    "            0.05, 0.95, \n",
    "            row_labels[label_key], \n",
    "            ax=axarr[row_idx, 0], \n",
    "            fontsize=label_config['fontsize_title'], \n",
    "            bordercolor='k', \n",
    "            color='w', \n",
    "            borderwidth=6\n",
    "        )\n",
    "\n",
    "# Add column headers to top row\n",
    "headers = viz_config['headers']\n",
    "if headers['col1']:\n",
    "    ek.text(\n",
    "        0.05, 0.95, \n",
    "        headers['col1'], \n",
    "        ax=axarr[0, 1], \n",
    "        fontsize=label_config['fontsize_header'], \n",
    "        bordercolor='k', \n",
    "        color='w', \n",
    "        borderwidth=6\n",
    "    )\n",
    "if headers['col2']:\n",
    "    ek.text(\n",
    "        0.05, 0.95, \n",
    "        headers['col2'], \n",
    "        ax=axarr[0, 2], \n",
    "        fontsize=label_config['fontsize_header'], \n",
    "        bordercolor='k', \n",
    "        color='w', \n",
    "        borderwidth=6\n",
    "    )\n",
    "\n",
    "# Remove ticks from all axes\n",
    "for ax in axarr.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=fig_config['wspace'], hspace=fig_config['hspace'])\n",
    "\n",
    "# Save figure\n",
    "output_dir = Path.cwd().parent.parent / fig_config['output_dir']\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "filename = fig_config['filename_pattern'].format(mode=mode)\n",
    "output_path = output_dir / filename\n",
    "\n",
    "plt.savefig(output_path, dpi=fig_config['dpi'], bbox_inches='tight')\n",
    "print(f\"✅ Figure saved to: {output_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "end",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
