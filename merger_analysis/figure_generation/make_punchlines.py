#!/usr/bin/env python3
"""
Generate Punchline Figures for Merger Analysis

This script creates all publication-quality figures from the merger classification
analysis, converting the exploratory notebook analysis into reproducible figure generation.

Features:
- Loads BYOL embeddings, PCA reductions, and label propagations
- Generates 9 key figures showing merger classifications and relationships
- Clean separation between data loading and figure generation
- Each figure is generated by its own function for readability

Usage:
    # Generate all figures using default config
    python make_punchlines.py

    # Specify custom config
    python make_punchlines.py --config ../custom_config.yaml

    # Specify output directory
    python make_punchlines.py --output-dir ../figures/

    # Generate only specific figures
    python make_punchlines.py --figures 1,2,3
"""

import os
import sys
import argparse
import logging
import pickle
import glob
from pathlib import Path
from typing import Dict, Tuple, Optional

import yaml
import numpy as np
import pandas as pd
import torch
import matplotlib
#matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import colors
from tqdm import tqdm

# Add pieridae to path
sys.path.insert(0, str(Path(__file__).parents[2]))

from pieridae.starbursts.byol import (
    EmbeddingAnalyzer,
    LabelPropagation,
    FrozenClassifier,
)
from pieridae.starbursts import sample
from ekfplot import plot as ek, colors as ec, colorlists
from ekfphys import calibrations
from ekfstats import sampling


def setup_logging(level: str = 'INFO') -> logging.Logger:
    """Setup logging configuration"""
    logger = logging.getLogger('make_punchlines')
    logger.setLevel(getattr(logging, level))

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    return logger


def load_config(config_path: str, input_path: Optional[str] = None) -> dict:
    """Load configuration from YAML file"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    config['data']['input_path'] = Path(config['data']['input_path'])
    config['data']['output_path'] = Path(config['data']['output_path'])

    # Override output_path if provided via command line
    if input_path is not None:
        config['data']['output_path'] = Path(input_path)
    #Path('../output/fiducial/')

    return config


def load_image_by_name(img_name: str, data_path: Path) -> np.ndarray:
    """
    Load a single galaxy image on-demand.

    Parameters
    ----------
    img_name : str
        Galaxy ID (e.g., 'M1234567890123456789')
    data_path : Path
        Base data directory path

    Returns
    -------
    image : np.ndarray
        Image array with shape (3, H, W) containing [g-band, i-band, hf_i-band]
    """
    i_file = data_path / img_name / f"{img_name}_i_results.pkl"
    g_file = data_path / img_name / f"{img_name}_g_results.pkl"

    img = []
    for band_file in [g_file, i_file]:
        with open(band_file, 'rb') as f:
            xf = pickle.load(f)
            img.append(xf['image'])
            if band_file == i_file:
                img.append(xf['hf_image'])

    return np.array(img, dtype=np.float32)


def load_data(config: dict, logger: logging.Logger,
              use_nn_classifier: bool = True) -> Dict:
    """
    Load all data needed for figure generation.

    This function consolidates all data loading operations from the notebook
    into a single place, including:
    - Embeddings and PCA reduction
    - Image names and labels
    - Label propagation results
    - Full catalog with adjusted masses
    - H-alpha morphology statistics
    - Satellite classifications

    Parameters
    ----------
    config : dict
        Configuration dictionary
    logger : logging.Logger
        Logger instance

    Returns
    -------
    data : dict
        Dictionary containing all loaded data
    """
    logger.info("=" * 60)
    logger.info("LOADING DATA")
    logger.info("=" * 60)

    data = {}

    # Get paths
    data_path = config['data']['input_path']
    output_path = config['data']['output_path']

    logger.info(f"Input path: {data_path}")
    logger.info(f"Output path: {output_path}")

    # Load image names
    logger.info("Loading image names...")
    pattern = f"{data_path}/M*/*i_results.pkl"
    filenames = glob.glob(pattern)

    valid_files = []
    for fname in filenames:
        g_file = fname.replace('_i_', '_g_')
        i_file = fname
        if os.path.exists(g_file) and os.path.exists(i_file):
            valid_files.append(fname)

    img_names = np.array([Path(fname).parent.name for fname in valid_files])
    data['img_names'] = img_names
    data['data_path'] = data_path
    logger.info(f"Found {len(img_names)} images")

    # Load embeddings
    logger.info("Loading embeddings...")
    embeddings_file = output_path / 'embeddings.npy'
    if not embeddings_file.exists():
        raise FileNotFoundError(f"Embeddings not found: {embeddings_file}")

    embeddings = np.load(embeddings_file)
    data['embeddings'] = embeddings
    logger.info(f"Embeddings shape: {embeddings.shape}")

    # Compute PCA
    logger.info("Computing PCA...")
    analyzer = EmbeddingAnalyzer(config)
    embeddings_pca = analyzer.compute_pca(embeddings)
    data['embeddings_pca'] = embeddings_pca
    explained_var = analyzer.pca.explained_variance_ratio_.sum() * 100
    logger.info(f"PCA complete: {analyzer.pca.n_components_} components, {explained_var:.1f}% variance")

    # Load labels
    logger.info("Loading classification labels...")
    label_file = Path(config.get('labels', {}).get('classifications_file', ''))

    if label_file.exists():
        mergers = pd.read_csv(label_file, index_col=0)
        labels = mergers.reindex(img_names)
        labels = labels.replace(np.nan, 0).values.flatten().astype(int)
        data['labels'] = labels

        label_meanings = config.get('labels', {}).get('label_mapping', {})
        data['label_meanings'] = label_meanings
        unique, counts = np.unique(labels, return_counts=True)

        logger.info("Label distribution:")
        for label_val, count in zip(unique, counts):
            meaning = label_meanings.get(label_val, f"unknown_{label_val}")
            logger.info(f"  {label_val} ({meaning}): {count} objects")
    else:
        logger.warning(f"Label file not found: {label_file}")
        labels = np.zeros(len(img_names), dtype=int)
        data['labels'] = labels
        data['label_meanings'] = {0: 'unclassified'}

    # Check if trained classifier is available
    logger.info("Checking for trained classifier...")
    model_path = Path(output_path) / 'byol_final_model.pt'
    

    if (model_path.exists()) and use_nn_classifier:
        try:
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
            if 'classifier_state_dict' in checkpoint:
                use_nn_classifier = True
                logger.info("Found trained classifier - using FrozenClassifier (neural network)")
        except Exception as e:
            logger.warning(f"Failed to load checkpoint: {e}")
            logger.info("Falling back to LabelPropagation")
            use_nn_classifier = False
        

    # Initialize appropriate classifier
    if use_nn_classifier:
        propagator = FrozenClassifier(
            model_path=model_path,
            config=config,
            logger=logger
        )
        # FrozenClassifier uses raw embeddings, not PCA
        input_embeddings = embeddings
    else:
        logger.info("Using LabelPropagation (K-NN based)")
        n_neighbors = config.get('labels', {}).get('n_neighbors', 50)
        n_min = config.get('labels', {}).get('minimum_labeled_neighbors', 5)
        n_min_auto = config.get('labels', {}).get('minimum_labeled_neighbors_for_autoprop', 10)

        propagator = LabelPropagation(
            n_neighbors=n_neighbors,
            n_min=n_min,
            n_min_auto=n_min_auto,
            prob_threshold=config.get('labels', {}).get('prob_threshold', 0.7),
            frag_threshold=config.get('labels', {}).get('frag_threshold', 0.25),
        )
        # LabelPropagation uses PCA embeddings
        input_embeddings = embeddings_pca

    # Run classification
    logger.info("Running classification...")
    iterative_labels, n_labels_iter, prob_labels_iter, stats = \
        propagator.iterative_propagation(input_embeddings, labels)

    data['iterative_labels'] = iterative_labels
    data['n_labels_iter'] = n_labels_iter
    data['prob_labels_iter'] = prob_labels_iter

    logger.info(f"Human labels: {stats['n_human']}")
    logger.info(f"Auto-labels added: {stats['n_added_iteration']}")
    logger.info(f"Total labels: {stats['n_final_auto']}")

    # Identify merger candidates
    logger.info("Identifying merger candidates...")
    fragmented = prob_labels_iter[:, 4] > 0.2
    pmerger = prob_labels_iter[:, 2] + prob_labels_iter[:, 3]
    possible_merger = pmerger > prob_labels_iter[:, 1]

    is_merger = possible_merger & ~fragmented
    is_undisturbed = ~possible_merger & ~fragmented

    data['fragmented'] = fragmented
    data['pmerger'] = pmerger
    data['possible_merger'] = possible_merger
    data['is_merger'] = is_merger
    data['is_undisturbed'] = is_undisturbed

    logger.info(f"Fragmented objects: {fragmented.sum()}")
    logger.info(f"Merger candidates: {is_merger.sum()}")

    # Load full catalog
    logger.info("Loading full catalog...")
    full_catalog, masks = sample.load_sample(
        '/Users/kadofong/work/projects/merian/local_data/base_catalogs/mdr1_n708maglt26_and_pzgteq0p1.parquet'
    )
    base_catalog = full_catalog.loc[masks['is_good'][0]]

    # Load adjusted masses
    logger.info("Loading adjusted masses from individual files...")
    for sid in tqdm(base_catalog.index, desc="Loading masses"):
        filename = f'{data_path}/{sid}/{sid}_i_results.pkl'
        if not os.path.exists(filename):
            continue
        with open(filename, 'rb') as f:
            x = pickle.load(f)
        base_catalog.loc[sid, 'logmass_adjusted'] = x['logmass_adjusted']

    base_catalog.loc[base_catalog['logmass_adjusted'].isna(), 'logmass_adjusted'] = \
        base_catalog.loc[base_catalog['logmass_adjusted'].isna(), 'logmass']

    # Create catalog subset
    fragmented_highthresh = prob_labels_iter[:, 4] > 0.3
    catalog = base_catalog.reindex(img_names[~fragmented_highthresh])
    catalog['p_merger'] = np.where(
        (prob_labels_iter[~fragmented_highthresh] == 0).all(axis=1),
        np.nan,
        prob_labels_iter[~fragmented_highthresh, 3]
    )
    catalog['p_ambig'] = np.where(
        (prob_labels_iter[~fragmented_highthresh] == 0).all(axis=1),
        np.nan,
        prob_labels_iter[~fragmented_highthresh, 2]
    )
    catalog['p_undisturbed'] = np.where(
        (prob_labels_iter[~fragmented_highthresh] == 0).all(axis=1),
        np.nan,
        prob_labels_iter[~fragmented_highthresh, 1]
    )

    dm = catalog['logmass_adjusted'] - catalog['logmass']
    catalog = catalog.loc[
        (dm < 0.5) &
        (catalog['logmass_adjusted'] <= 10.5) &
        (catalog['logmass_adjusted'] >= 7.5)
    ]

    data['catalog'] = catalog
    logger.info(f"Catalog size: {len(catalog)} objects")

    # Load H-alpha morphology
    logger.info("Loading H-alpha morphology statistics...")
    hamorph_file = '../../local_data/abby_morphology_statistics.csv'
    if os.path.exists(hamorph_file):
        hamorph = pd.read_csv(hamorph_file, index_col=0)
        hamorph['halpha_m20'] = np.where(hamorph['halpha_m20'] < -3, np.nan, hamorph['halpha_m20'])
        hamorph['continuum_m20'] = np.where(hamorph['continuum_m20'] < -3, np.nan, hamorph['continuum_m20'])
        data['hamorph'] = hamorph
        logger.info(f"Loaded morphology for {len(hamorph)} objects")
    else:
        logger.warning(f"H-alpha morphology file not found: {hamorph_file}")
        data['hamorph'] = None

    # Load satellite catalog
    logger.info("Loading satellite catalog...")
    cached_file = '../local_data/yue_meriansatellites_catalog/cached_catalog.csv'
    lucas_file = '../local_data/yue_meriansatellites_catalog/Merian_dwarf_hosts.csv'

    if os.path.exists(cached_file) and os.path.exists(lucas_file):
        yue = pd.read_csv(cached_file, index_col=0)
        lucas = pd.read_csv(lucas_file)
        lucas_trimmed = lucas[['objectId_Merian', 'coord_ra_Merian', 'coord_dec_Merian']]
        lucas_trimmed.index = [f'M{oid}' for oid in lucas_trimmed['objectId_Merian']]

        final_satellites = pd.concat([yue, lucas_trimmed])
        final_satellites = final_satellites.loc[~final_satellites.index.duplicated()]

        is_satellite = np.in1d(catalog.index, final_satellites.index)
        data['is_satellite'] = is_satellite
        logger.info(f"Identified {is_satellite.sum()} satellites")
    else:
        logger.warning("Satellite catalog not found")
        data['is_satellite'] = None

    logger.info("Data loading complete!")
    logger.info("=" * 60)

    return data


def make_figure_label_distribution(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 1: Label distribution histogram comparing manual vs auto-classification.

    Shows histogram of manual labels for objects classified as merger vs non-merger
    by the automatic classification system.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 1: Label distribution histogram")

    fig, ax = plt.subplots(1, 1, figsize=(5, 5))

    labels = data['labels']
    is_merger = data['is_merger']
    is_undisturbed = data['is_undisturbed']

    if (labels > 0).any():
        hist_kwargs = {
            'alpha': 0.7,
            'lw': 3,
            'bins': np.arange(0.5, 5.5),
            'density': True
        }

        ek.hist(labels[is_undisturbed], 
                ax=ax, 
                label='Not merger', 
                color=colorlists.slides['grey'],                
                **hist_kwargs)
        ek.hist(
            labels[is_merger], 
            ax=ax, 
            label='Merger', 
            color=colorlists.slides['blue'],
            hatch='//',
            **hist_kwargs
        )

        ax.set_xticks(np.arange(1, 5), ['undisturbed', 'ambiguous', 'merger', 'fragmented'], rotation=35)
        ax.set_xlabel('Manual labels')
        ax.set_ylabel('Density')
        ax.legend(title='Learned classification')

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig1_label_distribution.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")

def make_figure_classification_conflicts(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger,
    n_examples: int = 6
) -> None:
    """
    Figure 2: Example galaxies where auto and manual classifications conflict.

    Shows galaxies with low merger probability but manually labeled as non-merger,
    displaying i-band, i-band (LSB), and high-frequency images.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    n_examples : int
        Number of examples to show
    """
    logger.info("Generating Figure 2: Classification conflicts")

    img_names = data['img_names']
    labels = data['labels']
    prob_labels_iter = data['prob_labels_iter']
    n_labels_iter = data['n_labels_iter']
    pmerger = data['pmerger']
    fragmented = data['fragmented']
    data_path = data['data_path']
    label_meanings = data['label_meanings']

    manual_merger = (labels == 3) | (labels == 2)
    manual_nonmerger = labels == 1

    is_automerger = pmerger > 0.4
    mu_am = np.arange(len(img_names))[is_automerger & ~fragmented & manual_nonmerger]
    mm_au = np.arange(len(img_names))[~is_automerger & ~fragmented & manual_merger]
    
    n_examples = min(n_examples, len(mu_am))

    if n_examples == 0:
        logger.warning("No classification conflicts found")
        return

    np.random.seed(123403)
    example_indices = np.concatenate([
        np.random.choice(mu_am, n_examples//2, replace=False),
        np.random.choice(mm_au, n_examples//2, replace=False),
    ])
    print(example_indices)
    fig, axarr = plt.subplots(3, n_examples, figsize=( n_examples * 2.3, 7.))
    axarr = axarr.T

    for idx, gix in enumerate(example_indices):
        img_name = img_names[gix]
        image = load_image_by_name(img_name, data_path)

        # i-band
        ek.imshow(image[1], ax=axarr[idx, 0], q=0.01, cmap='Greys')

        # i-band log scale
        axarr[idx, 1].imshow(
            image[1],
            origin='lower',
            cmap='Greys',
            norm=colors.SymLogNorm(linthresh=0.1)
        )

        # High-frequency
        ek.imshow(image[2], ax=axarr[idx, 2], cmap='magma')

        # Add statistics
        ek.text(
            0.025, 0.025,
            f"""N_labels = {n_labels_iter[gix]}
Pr[ud] = {prob_labels_iter[gix, 1]:.2f}
Pr[amb] = {prob_labels_iter[gix, 2]:.2f}
Pr[merg] = {prob_labels_iter[gix, 3]:.2f}
Pr[frag] = {prob_labels_iter[gix, 4]:.2f}""",
            ax=axarr[idx, 0],
            fontsize=9,
            bordercolor='w',
            color='k',
            borderwidth=3
        )
        ek.text(
            0.025,
            0.025,
            f'Human label: {label_meanings[labels[gix]]}',
            ax=axarr[idx, 1],
            fontsize=9,
            bordercolor='w',
            color= (labels[gix]==1) and 'k' or colorlists.slides['blue'],
            borderwidth=3
        )

    for ax in axarr.flatten():
        ax.set_xticks([])
        ax.set_yticks([])

    for ax in axarr[:3,:].flatten():
        ek.set_framecolor(colorlists.slides['blue'], ax=ax)
    for ax in axarr[3:,:].flatten():
        ek.set_framecolor(colorlists.slides['grey'], ax=ax)

    ek.text(0.05, 0.95, 'HSC i-band', ax=axarr[0, 0], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)
    ek.text(0.05, 0.95, 'HSC i-band (LSB)', ax=axarr[0, 1], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)
    ek.text(0.05, 0.95, 'Starlet HF', ax=axarr[0, 2], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig2_classification_conflicts.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_merger_candidates(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger,
    n_examples: int = 6
) -> None:
    """
    Figure 3: Example merger candidate galaxies.

    Shows galaxies identified as possible mergers by the automatic classification,
    displaying i-band, i-band (LSB), and high-frequency images.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    n_examples : int
        Number of examples to show
    """
    logger.info("Generating Figure 3: Merger candidates")

    img_names = data['img_names']
    prob_labels_iter = data['prob_labels_iter']
    n_labels_iter = data['n_labels_iter']
    possible_merger = data['possible_merger']
    fragmented = data['fragmented']
    data_path = data['data_path']

    candidates = np.arange(len(img_names))[possible_merger & ~fragmented]
    n_examples = min(n_examples, len(candidates))

    if n_examples == 0:
        logger.warning("No merger candidates found")
        return

    example_indices = np.random.choice(candidates, n_examples, replace=False)

    fig, axarr = plt.subplots(3, n_examples, figsize=(4 * n_examples, 10))

    for idx, gix in enumerate(example_indices):
        img_name = img_names[gix]
        image = load_image_by_name(img_name, data_path)

        # i-band
        ek.imshow(image[1], ax=axarr[0, idx], q=0.01, cmap='Greys')

        # i-band log scale
        axarr[1, idx].imshow(
            image[1],
            origin='lower',
            cmap='Greys',
            norm=colors.SymLogNorm(linthresh=0.1)
        )

        # High-frequency
        ek.imshow(image[2], ax=axarr[2, idx], cmap='magma')

        # Add statistics
        ek.text(
            0.025, 0.025,
            f"""N_labels = {n_labels_iter[gix]}
Pr[ud] = {prob_labels_iter[gix, 1]:.2f}
Pr[amb] = {prob_labels_iter[gix, 2]:.2f}
Pr[merg] = {prob_labels_iter[gix, 3]:.2f}
Pr[frag] = {prob_labels_iter[gix, 4]:.2f}""",
            ax=axarr[0, idx],
            fontsize=9,
            bordercolor='w',
            color='k',
            borderwidth=3
        )

    for ax in axarr.flatten():
        ax.set_xticks([])
        ax.set_yticks([])

    ek.text(0.05, 0.95, 'HSC i-band', ax=axarr[0, 0], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)
    ek.text(0.05, 0.95, 'HSC i-band (LSB)', ax=axarr[1, 0], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)
    ek.text(0.05, 0.95, 'Starlet HF', ax=axarr[2, 0], fontsize=12,
            bordercolor='k', color='w', borderwidth=6)

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig3_merger_candidates.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

    logger.info(f"Saved: {output_file}")


def make_figure_ha_sfs_merger_fraction(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 4: H-alpha luminosity vs stellar mass with merger probability overlay.

    Shows the star-forming sequence with color-coded average merger probability
    and scatter plot of high-confidence merger candidates.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 4: H-alpha vs M* with merger fraction")

    catalog = data['catalog']

    fig, axarr = plt.subplots(1, 2, figsize=(10, 4))

    bins = [np.logspace(7.75, 10.5, 15), np.logspace(39, 41.9, 20)]
    
    cmap = ec.colormap_from_list([
        ec.ColorBase(colorlists.slides['orange']).modulate(-0.3,0.3).base, 
        ec.ColorBase(colorlists.slides['orange']).modulate(0.,-0.3).base, 
        plt.cm.coolwarm(0.5), 
        ec.ColorBase(colorlists.slides['bluebird']).modulate(0.3,-0.3).base, 
        ec.ColorBase(colorlists.slides['bluebird']).modulate(-0.1,0.5).base,    
    ])
    # Left panel: average merger probability
    im, _ = ek.pcolor_avg2d(
        10.**catalog['logmass_adjusted'],
        catalog['L_Ha'],
        catalog['p_merger'] + catalog['p_ambig'],
        cmap=cmap,
        yscale='log',
        xscale='log',
        zscale='log',
        bins=bins,
        ax=axarr[0],
    )

    # Right panel: number count histogram with scatter
    imx = ek.hist2d(
        10.**catalog['logmass_adjusted'],
        catalog['L_Ha'],
        cmap=ec.ColorBase('k').sequential_cmap(fade=1.),
        yscale='log',
        xscale='log',
        bins=bins,
        ax=axarr[1],
    )

    probable_merger = (catalog['p_ambig'] + catalog['p_merger']) > np.nanquantile((catalog['p_ambig'] + catalog['p_merger']), 0.975)
    axarr[1].scatter(
        10.**catalog.loc[probable_merger, 'logmass_adjusted'],
        catalog.loc[probable_merger, 'L_Ha'],
        fc=cmap(.95),
        ec=cmap(0.8),
        s=4**2,
        #label=r'Pr[interaction] > Pr[undisturbed]'
    )
    #axarr[1].legend(loc='lower right', fontsize=10)

    # Plot star-forming sequence
    ms = im._coordinates.data[0, :, 0]
    alpha = -0.13 * 0.08 + 0.8
    norm = 1.24 * 0.08 - 1.47
    sfs = 10.**(alpha * (np.log10(ms) - 8.5) + norm)
    ha_sfs = calibrations.SFR2LHa(sfs)

    plt.colorbar(imx[0][-1], ax=axarr[1], label=r'$\langle N_{\rm merger} \rangle$')
    plt.colorbar(im, ax=axarr[0], label=r'$\langle {\rm Pr[interaction]}\rangle$')

    for ax in axarr:
        ek.outlined_plot(
            ms,
            ha_sfs,
            ax=ax,
            lw=1,
            ls='--',
        )

        ek.text(
            7e7,
            3e39,
            'EKF+24b SFS',
            ax=ax,
            rotation=37,
            coord_type='absolute',
            va='bottom',
            ha='left',
            bordercolor='w',
            borderwidth=2,
            fontsize=8,
        )
        ax.set_xlabel(ek.common_labels['mstar'])
        ax.set_ylabel(ek.common_labels['halum'])
        ek.loglog(ax=ax)

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig4_ha_sfs_merger_fraction.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_merger_fraction_vs_mass(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 5: Merger detection fraction as a function of stellar mass.

    Shows the baseline merger probability and high-confidence merger fraction
    for galaxies near the star-forming sequence.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 5: Merger fraction vs mass")

    catalog = data['catalog']

    # Compute SFS relation
    alpha = -0.13 * 0.08 + 0.8
    norm = 1.24 * 0.08 - 1.47
    sfs_std = 0.22 * 0.08 + 0.38
    sfs = lambda logmstar: alpha * (logmstar - 8.5) + norm

    # Compute distance from SFS
    dsfs = np.log10(calibrations.LHa2SFR(catalog['L_Ha'])) - sfs(catalog['logmass_adjusted'])
    mask = abs(dsfs / sfs_std) < 0.2

    pmerger = catalog['p_merger'] + catalog['p_ambig']

    out_baseline = sampling.running_metric(
        catalog.loc[mask, 'logmass_adjusted'],
        pmerger.loc[mask],
        np.nanmean,
        np.linspace(7., 10.25, 12),
        erronmetric=True
    )

    out_highconf = sampling.running_metric(
        catalog.loc[mask, 'logmass_adjusted'],
        (pmerger > catalog['p_undisturbed']).loc[mask],
        np.nanmean,
        np.linspace(7., 10.25, 8),
        erronmetric=True
    )

    fig, ax = plt.subplots(figsize=(6, 5))

    ek.errorbar(
        out_baseline[0].flatten(),
        out_baseline[1][:, 0, 2].flatten(),
        ylow=out_baseline[1][:, 0, 1],
        yhigh=out_baseline[1][:, 0, 3],
        label=r'Pr[interaction]-weighted',
        color=colorlists.slides['blue']
    )
    ek.errorbar(
        out_highconf[0].flatten(),
        out_highconf[1][:, 0, 2].flatten(),
        ylow=out_highconf[1][:, 0, 1],
        yhigh=out_highconf[1][:, 0, 3],
        label='High-confidence mergers',
        color=colorlists.slides['orange']
    )

    ax.legend()
    ax.set_yscale('log')
    ax.set_xlabel(ek.common_labels['logmstar'])
    ax.set_ylabel('Merger detection fraction')

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig5_merger_fraction_vs_mass.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_merger_prob_vs_dsfs(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 6: Merger probability vs distance from star-forming sequence, by mass bin.

    Shows how merger probability varies with SFR offset from the SFS,
    with separate curves for different stellar mass bins.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 6: Merger probability vs dSFS by mass")

    catalog = data['catalog']

    # Compute SFS relation
    alpha = -0.13 * 0.08 + 0.8
    norm = 1.24 * 0.08 - 1.47
    sfs_std = 0.22 * 0.08 + 0.38
    sfs = lambda logmstar: alpha * (logmstar - 8.5) + norm

    pmerger = catalog['p_merger'] + catalog['p_ambig']

    # Compute baseline merger probability as function of mass
    dsfs = np.log10(calibrations.LHa2SFR(catalog['L_Ha'])) - sfs(catalog['logmass_adjusted'])
    mask = abs(dsfs / sfs_std) < 0.2

    out_baseline = sampling.running_metric(
        catalog.loc[mask, 'logmass_adjusted'],
        pmerger.loc[mask],
        np.nanmean,
        np.linspace(7., 10.25, 12),
        erronmetric=True
    )
    pmerger_baseline_by_mass = lambda logmstar: np.interp(
        logmstar,
        out_baseline[0].flatten(),
        out_baseline[1][:, 0, 2].flatten()
    )

    fig, axarr = plt.subplots(1, 2, figsize=(12, 5))

    logmstar_bins = [7.5] + list(np.arange(8.5, 10., 0.3)) + [12]
    groups = np.digitize(catalog['logmass_adjusted'], logmstar_bins)
    groupids = np.arange(1,len(logmstar_bins))
    cmap = ec.colormap_from_list([colorlists.slides['orange'], plt.cm.coolwarm(0.5), colorlists.slides['bluebird']])

    axarr[0].set_xlim(-0.75, 4.5)
    axarr[0].set_ylim(0., 1.)
    
    for gidx, gid in enumerate(groupids):
        selected = catalog.loc[groups == gid]
        
        for idx, is_normalized in enumerate([False, True]):

            ms_at_mass = sfs(selected['logmass_adjusted'])
            dsfs = np.log10(calibrations.LHa2SFR(selected['L_Ha'])) - ms_at_mass
            assns, loglhabins = sampling.bin_by_count(dsfs, 10, 0.25)
            xs = sampling.midpts(loglhabins) / sfs_std

            if is_normalized:
                factor = 1. / pmerger_baseline_by_mass(selected['logmass_adjusted'])
            else:
                factor = 1.

            _, ys, _ = sampling.running_metric(
                dsfs,
                pmerger.reindex(selected.index) * factor,
                np.nanmean,
                sampling.midpts(loglhabins),
                erronmetric=True
            )

            ek.outlined_plot(
                xs,
                ys[:, 0, 2],
                lw=2,
                ax=axarr[idx],
                color=cmap(gidx / len(groupids))
            )
            axarr[idx].fill_between(
                xs,
                ys[:, 0, 1],
                ys[:, 0, 3],
                label=f'[{logmstar_bins[gid-1]:.2f},{logmstar_bins[gid]:.2f}]',
                alpha=0.3,
                color=cmap(gidx / len(groupids))
            )
            
            
            if not is_normalized:
                if gid < 3:
                    offset = 2
                elif gid < 4:
                    offset=1
                else:
                    offset = 0
                slope = (ys[3+offset,0,2]-ys[2+offset,0,2])/(xs[3+offset]-xs[2+offset])
                
                ek.text(
                    sampling.midpts(xs[2+offset:4+offset]),
                    sampling.midpts(ys[2+offset:4+offset,0,2]),
                    rf'[{logmstar_bins[gid-1]:.1f},{logmstar_bins[gid]:.1f}]',
                    ha='center',
                    va='center',
                    rotation = np.rad2deg(np.arctan(slope * ek.get_subplot_aspectratio(axarr[idx]))),
                    coord_type='absolute',
                    ax=axarr[idx],
                    color=cmap(gidx / len(groupids)),
                    bordercolor='w',
                    borderwidth=3,
                    fontsize=11
                )

    # Add overall trend to normalized panel
    xs = (np.log10(calibrations.LHa2SFR(catalog['L_Ha'])) - sfs(catalog['logmass_adjusted'])) / sfs_std
    ys = pmerger / pmerger_baseline_by_mass(catalog['logmass_adjusted'])
    out = sampling.running_metric(xs, ys, np.nanmean, np.arange(-0.5, 4.5, 0.2), dx=0.4, erronmetric=True)
    axarr[1].fill_between(
        out[0],
        out[1][:, 0, 1],
        out[1][:, 0, 3],
        color='grey',
        alpha=0.4,
    )
    ek.outlined_plot(
        out[0],
        out[1][:, 0, 2],
        ax=axarr[1],
        ls='--',
        lw=2
    )

    for ax in axarr:
        ax.set_xlabel(r'$ \mathcal{S} = \frac{\log_{10}[{\rm SFR}/{\rm SFS(M_\bigstar)}]}{\sigma_{\rm SFS}}$', fontsize=20)
    axarr[0].set_ylabel(r'$\langle \rm Pr[interaction] \rangle$')
    axarr[1].set_ylabel(r'$\mathcal{R}_{\rm int}(\mathcal{S})$')
    axarr[1].set_ylim(0.5, axarr[1].get_ylim()[-1])
    axarr[1].set_yscale('log')

    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig6_merger_prob_vs_dsfs.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_hamorph_distributions(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 7: H-alpha morphology distributions.

    Shows distributions of asymmetry, Gini, and M20 for H-alpha emission,
    comparing unweighted, merger-probability-weighted, and high-confidence mergers.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 7: H-alpha morphology distributions")

    catalog = data['catalog']
    hamorph = data['hamorph']

    if hamorph is None:
        logger.warning("H-alpha morphology data not available, skipping")
        return

    pmerger = catalog['p_merger'] + catalog['p_ambig']
    pmerger_threshold = np.nanquantile((catalog['p_ambig'] + catalog['p_merger']), 0.975)

    fig, axarr = plt.subplots(1, 3, figsize=(12, 4))

    tags = {'continuum': 'continuum', 'halpha': r'H$\alpha$'}
    labels = ['Asymmetry', r'G', r'$M_{20}$']
    keys = ['asymmetry', 'gini', 'm20']

    prefix = 'halpha'

    for idx, key in enumerate(keys):
        morph_key = f'{prefix}_{key}'
        out = ek.hist(
            hamorph.reindex(catalog.index)[morph_key],
            density=True,
            alpha=0.2,
            lw=2,
            color=ec.ColorBase(colorlists.slides['grey']).base,
            hatch='//',
            label='Unweighted',
            ax=axarr[idx],
            binalpha=0.005
        )
        bins = out[1][1]
        ek.hist(
            hamorph.reindex(catalog.index)[morph_key],
            weights=pmerger,
            density=True,
            alpha=0.4,
            lw=2.,
            color=colorlists.slides['blue'],
            label='Weighted by Pr[interaction]',
            ax=axarr[idx],
            bins=bins
        )
        ek.hist(
            hamorph.reindex(catalog.loc[(pmerger > pmerger_threshold)].index)[morph_key],
            density=True,
            alpha=0.4,
            lw=2,
            color=colorlists.slides['orange'],
            label='High-confidence mergers',
            ax=axarr[idx],
            bins=bins
        )
        if idx == 0:
            ek.text(0.025, 0.975, 'Unweighted', color='grey', ax=axarr[idx], fontsize=11)
            ek.text(0.025, 0.9, '''Weighted by
Pr[interaction]''', color=colorlists.slides['blue'], ax=axarr[idx], fontsize=11)
            ek.text(0.025, 0.75, '''High-confidence
mergers''', color=colorlists.slides['orange'], ax=axarr[idx], fontsize=11)
        axarr[idx].set_xlabel(rf'{labels[idx]}({tags[prefix]})')
        if idx == 0:
            axarr[idx].set_ylabel('PDF')

    plt.tight_layout()
    plt.subplots_adjust(wspace=0.15)
    if output_dir is not None:
        output_file = output_dir / 'fig7_hamorph_distributions.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_hamorph_distributions_by_mass(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger,
    mass_bins: list = None
) -> None:
    """
    Figure 7b: H-alpha morphology distributions by mass bin.

    Shows distributions of asymmetry, Gini, and M20 for H-alpha emission,
    comparing unweighted, merger-probability-weighted, and high-confidence mergers,
    with each row representing a different stellar mass bin.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    mass_bins : list, optional
        Bin edges for logmstar_adjusted. Default is [7.5, 8.5, 9.5, 10.5]
    """
    logger.info("Generating Figure 7b: H-alpha morphology distributions by mass")

    catalog = data['catalog']
    hamorph = data['hamorph']

    if hamorph is None:
        logger.warning("H-alpha morphology data not available, skipping")
        return

    # Default mass bins
    if mass_bins is None:
        mass_bins = [7.5, 9., 10.5]

    n_mass_bins = len(mass_bins) - 1
    pmerger = catalog['p_merger'] + catalog['p_ambig']
    pmerger_threshold = np.nanquantile((catalog['p_ambig'] + catalog['p_merger']), 0.975)

    fig, axarr = plt.subplots(n_mass_bins, 3, figsize=(12, 3.5 * n_mass_bins))

    # Ensure axarr is 2D even if only one mass bin
    if n_mass_bins == 1:
        axarr = axarr.reshape(1, -1)

    tags = {'continuum': 'continuum', 'halpha': r'H$\alpha$'}
    labels = ['Asymmetry', r'G', r'$M_{20}$']
    keys = ['asymmetry', 'gini', 'm20']
    

    prefix = 'halpha'
    from ekfphys import calibrations # XXX
    # Loop through mass bins
    for mass_idx in range(n_mass_bins):
        mass_min = mass_bins[mass_idx]
        mass_max = mass_bins[mass_idx + 1]

        # Select galaxies in this mass bin
        mass_mask = (catalog['logmass_adjusted'] >= mass_min) & (catalog['logmass_adjusted'] < mass_max)
        catalog_bin = catalog.loc[mass_mask]
        pmerger_bin = pmerger.loc[mass_mask]        
        #pmerger_bin = np.log10(calibrations.LHa2SFR(catalog_bin['L_Ha'])) - catalog_bin['logmass_adjusted']
        #pmerger_bin = (pmerger_bin - pmerger_bin.min())/(pmerger_bin.max()-pmerger_bin.min())
        #print(pmerger_bin)
        logger.info(f"  Mass bin [{mass_min}, {mass_max}): {len(catalog_bin)} galaxies")

        # Loop through morphology parameters
        for idx, key in enumerate(keys):
            morph_key = f'{prefix}_{key}'

            # Unweighted histogram
            out = ek.hist(
                hamorph.reindex(catalog_bin.index)[morph_key],
                density=True,
                alpha=0.2,
                lw=2,
                color=ec.ColorBase(colorlists.slides['grey']).base,
                hatch='//',
                label='Unweighted',
                ax=axarr[mass_idx, idx],
                binalpha=0.01,
                bins=15,
            )
            bins = out[1][1]

            # Weighted by merger probability
            ek.hist(
                hamorph.reindex(catalog_bin.index)[morph_key],
                weights=pmerger_bin,
                density=True,
                alpha=0.4,
                lw=2.,
                color=colorlists.slides['blue'],
                label='Weighted by Pr[interaction]',
                ax=axarr[mass_idx, idx],
                bins=bins
            )

            # High-confidence mergers only
            high_conf_mask = pmerger_bin > pmerger_threshold
            if high_conf_mask.sum() > 0:
                ek.hist(
                    hamorph.reindex(catalog_bin.loc[high_conf_mask].index)[morph_key],
                    density=True,
                    alpha=0.4,
                    lw=2,
                    color=colorlists.slides['orange'],
                    label='High-confidence mergers',
                    ax=axarr[mass_idx, idx],
                    bins=bins
                )

            # Add legend/labels to first column only
            if (idx == 0):
                if mass_idx == 0:
                    ek.text(0.025, 0.975, 'Unweighted', color='grey', ax=axarr[mass_idx, idx], fontsize=11)
                    ek.text(0.025, 0.9, '''Weighted by
    Pr[interaction]''', color=colorlists.slides['blue'], ax=axarr[mass_idx, idx], fontsize=11)                    
                if (mass_idx == (n_mass_bins-1)):
                    ek.text(0.025, 0.975, '''High-confidence
    mergers''', color=colorlists.slides['orange'], ax=axarr[mass_idx, idx], fontsize=11)

                # Add mass bin label
                ek.text(0.975, 0.975, f'${mass_min:.1f} < \\log M_* < {mass_max:.1f}$',
                       ax=axarr[mass_idx, idx], fontsize=12, ha='right')

            # Set x-label (only on bottom row)
            #if mass_idx == n_mass_bins - 1:
            axarr[mass_idx, idx].set_xlabel(rf'{labels[idx]}({tags[prefix]})')

            # Set y-label (only on first column)
            if idx == 0:
                axarr[mass_idx, idx].set_ylabel('PDF')

    plt.tight_layout()
    #plt.subplots_adjust(wspace=0.15, hspace=0.2)
    if output_dir is not None:
        output_file = output_dir / 'fig7b_hamorph_distributions_by_mass.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def make_figure_gini_m20_space(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 8: Gini-M20 diagram with merger probability.

    Shows the distribution of galaxies in Gini-M20 space with color-coded
    merger probability, including Lotz et al. classification boundaries.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 8: Gini-M20 diagram")

    catalog = data['catalog']
    hamorph = data['hamorph']

    if hamorph is None:
        logger.warning("H-alpha morphology data not available, skipping")
        return

    pmerger = catalog['p_merger'] + catalog['p_ambig']

    fig, ax = plt.subplots(figsize=(7, 6))
    cmap = ec.colormap_from_list([
        ec.ColorBase(colorlists.slides['orange']).modulate(-0.3,-0.1).base, 
        colorlists.slides['orange'], 
        plt.cm.coolwarm(0.5), 
        colorlists.slides['bluebird'],
        ec.ColorBase(colorlists.slides['bluebird']).modulate(0.3,0.3).base, 
    ])
    
    ek.pcolor_avg2d(
        hamorph.reindex(catalog.index)['continuum_m20'],
        hamorph.reindex(catalog.index)['continuum_gini'],
        pmerger,
        bins=[np.linspace(-2.5, -0.5, 30), np.linspace(0.3, 0.8, 30)],
        cmap=cmap,
        vmin=0.,
        vmax=0.16
    )
    ek.density_contour(
        hamorph.reindex(catalog.index)['continuum_m20'],
        hamorph.reindex(catalog.index)['continuum_gini'],
        quantiles=[0.16, 0.5, 0.9, 0.95, 0.975],
        colors='w'
    )

    # Lotz et al. classification boundaries
    lotza = np.array([[-3.0065789473684212, 0.3799999999999999],
                      [-1.6776315789473686, 0.56375]])
    lotzb = np.array([[-3.0000000000000004, 0.75],
                      [-0.0065789473684203514, 0.33124999999999993]])

    ek.outlined_plot(lotza[:, 0], lotza[:, 1], ls=':', lw=1, color='grey')
    ek.outlined_plot(lotzb[:, 0], lotzb[:, 1], ls=':', lw=1, color='grey')

    ax.set_xlabel("M20")
    ax.set_ylabel("Gini")
    ax.set_xlim(-2.5, -0.5)
    ax.set_ylim(0.3, 0.75)

    plt.colorbar(ax.collections[0], ax=ax, label=r'$\langle \rm Pr[interaction] \rangle$')
    plt.tight_layout()
    output_file = output_dir / 'fig8_gini_m20_space.pdf'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()

    logger.info(f"Saved: {output_file}")


def make_figure_merger_prob_vs_environment(
    data: Dict,
    output_dir: Path,
    logger: logging.Logger
) -> None:
    """
    Figure 9: Merger probability vs environment.

    Shows merger probability as a function of SFR offset from SFS,
    comparing satellites and centrals/field galaxies.

    Parameters
    ----------
    data : dict
        Data dictionary from load_data()
    output_dir : Path
        Output directory for figures
    logger : logging.Logger
        Logger instance
    """
    logger.info("Generating Figure 9: Merger probability vs environment")

    catalog = data['catalog']
    is_satellite = data['is_satellite']

    if is_satellite is None:
        logger.warning("Satellite data not available, skipping")
        return

    # Compute SFS relation
    alpha = -0.13 * 0.08 + 0.8
    norm = 1.24 * 0.08 - 1.47
    sfs_std = 0.22 * 0.08 + 0.38
    sfs = lambda logmstar: alpha * (logmstar - 8.5) + norm

    pmerger = catalog['p_merger'] + catalog['p_ambig']

    # Compute baseline merger probability
    dsfs = np.log10(calibrations.LHa2SFR(catalog['L_Ha'])) - sfs(catalog['logmass_adjusted'])
    mask = abs(dsfs / sfs_std) < 0.2

    out_baseline = sampling.running_metric(
        catalog.loc[mask, 'logmass_adjusted'],
        pmerger.loc[mask],
        np.nanmean,
        np.linspace(7., 10.25, 12),
        erronmetric=True
    )
    pmerger_baseline_by_mass = lambda logmstar: np.interp(
        logmstar,
        out_baseline[0].flatten(),
        out_baseline[1][:, 0, 2].flatten()
    )

    fig, axarr = plt.subplots(1,2, figsize=(10,5))

    xs = (np.log10(calibrations.LHa2SFR(catalog['L_Ha'])) - sfs(catalog['logmass_adjusted'])) / sfs_std
    ys = pmerger / pmerger_baseline_by_mass(catalog['logmass_adjusted'])

    ax = axarr[1]
    for envkey in [0, 1]:
        if envkey == 0:
            envmask = ~is_satellite
            env_indices = sampling.make_matched_sample(     
                catalog.loc[~is_satellite, 'logmass_adjusted'],
                catalog.loc[is_satellite, 'logmass_adjusted'],  
                #n_sample=5*is_satellite.sum()
            ).index
            #env_indices = catalog.loc[envmask].index
        elif envkey == 1:
            envmask = is_satellite
            env_indices = catalog.loc[envmask].index

        out = sampling.running_metric(
            xs.reindex(env_indices),
            ys.reindex(env_indices),
            np.nanmean,
            np.arange(-0.5, 3.75, 0.4),
            dx=0.5,
            erronmetric=True
        )
        cc = [colorlists.slides['blue'], colorlists.slides['red']][envkey]
        ax.fill_between(
            out[0],
            out[1][:, 0, 1],
            out[1][:, 0, 3],
            color=cc,
            alpha=0.4,
            hatch=envkey == 1 and '||' or None,
        )
        ek.outlined_plot(
            out[0],
            out[1][:, 0, 2],
            ax=ax,
            color=cc,
            lw=2,
            label=['Field? (sat mass-matched)', 'Satellite'][envkey]
        )
        #ek.hist(
        #    catalog.reindex(env_indices)['logmass_adjusted'],
        #    color=cc,
        #    alpha=0.3,
        #    ax=axarr[2],
        #    density=True
        #)

    ax.set_xlabel(r'$\mathcal{S}\equiv \frac{\log_{10}[{\rm SFR}/{\rm SFS(M_\bigstar)}]}{\sigma_{\rm SFS}}$', fontsize=20)
    #ax.set_ylabel(r'$\mathcal{q} \equiv \langle {\rm Pr[interaction]}\rangle/\langle \rm Pr[interaction|SFS] \rangle$')
    ax.set_ylabel(r'Excess mean interaction probability')
    ax.axhline(1., color='lightgrey', ls=':')
    
    ax.legend()

    # Top panel: SFR offset distributions
    xbins = np.linspace(min(out[0]), max(out[0]), 30)
    cc = [colorlists.slides['blue'], colorlists.slides['red']]
    lbls = ['Field?', 'Satellite']
    for envidx, mask in enumerate([~is_satellite, is_satellite]):
        hcounts = sampling.bootstrap_histcounts(
            xs[mask],
            bins=xbins
        )
        cumulative_hist = np.cumsum(hcounts, axis=1)/np.sum(hcounts,axis=1).reshape(-1,1)
        if envidx == 0:
            nrml = np.quantile(cumulative_hist, 0.5, axis=0)
        
        axarr[0].fill_between(
            sampling.midpts(xbins),
            np.quantile(cumulative_hist, 0.16, axis=0)/nrml,
            np.quantile(cumulative_hist, 0.84, axis=0)/nrml,
            alpha=0.3,
            color = cc[envidx],
            step='mid'
        )
        axarr[0].step(
            sampling.midpts(xbins),
            np.quantile(cumulative_hist, 0.5, axis=0)/nrml,
            lw=2, 
            color=cc[envidx],
            where='mid',
            label=lbls[envidx]
        )
    axarr[0].set_ylabel(r'$N(<\mathcal{S})/N_{\rm tot}$')
    axarr[0].set_xlabel(r'$\mathcal{S}\equiv \frac{\log_{10}[{\rm SFR}/{\rm SFS(M_\bigstar)}]}{\sigma_{\rm SFS}}$', fontsize=20)
    axarr[0].legend()

    for ax in axarr:
        ax.grid(axis='x', color='lightgrey')
    plt.tight_layout()
    if output_dir is not None:
        output_file = output_dir / 'fig9_merger_prob_vs_environment.pdf'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved: {output_file}")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Generate punchline figures from merger analysis',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate all figures using default config
  python make_punchlines.py

  # Use custom config
  python make_punchlines.py --config ../custom_config.yaml

  # Specify output directory
  python make_punchlines.py --output-dir ../figures/

  # Generate only specific figures (comma-separated)
  python make_punchlines.py --figures 1,2,3
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        default='../config.yaml',
        help='Path to configuration YAML file'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='./punchline_figures/',
        help='Output directory for figures'
    )

    parser.add_argument(
        '--input-path',
        type=str,
        default=None,
        help='Path to input data directory (overrides config["data"]["output_path"])'
    )

    parser.add_argument(
        '--figures',
        type=str,
        help='Comma-separated list of figure numbers to generate (1-9). If not specified, generates all.'
    )

    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility'
    )

    args = parser.parse_args()

    # Setup logging
    logger = setup_logging('INFO')

    try:
        # Set random seed
        np.random.seed(args.seed)

        # Load configuration
        config = load_config(args.config, input_path=args.input_path)
        logger.info(f"Configuration loaded from: {args.config}")
        if args.input_path:
            logger.info(f"Input path overridden to: {args.input_path}")

        # Create output directory
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Output directory: {output_dir}")

        # Load data
        data = load_data(config, logger)

        # Determine which figures to generate
        if args.figures:
            figure_nums = [int(x.strip()) for x in args.figures.split(',')]
        else:
            figure_nums = list(range(1, 10))

        logger.info("=" * 60)
        logger.info(f"GENERATING FIGURES: {figure_nums}")
        logger.info("=" * 60)

        # Generate figures
        figure_functions = {
            1: make_figure_label_distribution,
            2: make_figure_classification_conflicts,
            3: make_figure_merger_candidates,
            4: make_figure_ha_sfs_merger_fraction,
            5: make_figure_merger_fraction_vs_mass,
            6: make_figure_merger_prob_vs_dsfs,
            7: make_figure_hamorph_distributions,
            8: make_figure_gini_m20_space,
            9: make_figure_merger_prob_vs_environment,
        }

        for fig_num in figure_nums:
            if fig_num in figure_functions:
                try:
                    figure_functions[fig_num](data, output_dir, logger)
                except Exception as e:
                    print(f'Figure {fig_num} failed: {e}')
            else:
                logger.warning(f"Unknown figure number: {fig_num}")

        logger.info("=" * 60)
        logger.info("SUCCESS")
        logger.info("=" * 60)
        print(f"\n Figure generation completed successfully!")
        print(f"   Output directory: {output_dir}")

    except Exception as e:
        logger.error(f"Error during figure generation: {e}", exc_info=True)
        print(f"\n Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
