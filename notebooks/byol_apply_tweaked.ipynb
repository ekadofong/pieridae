{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfa25a3-cc2c-4716-9fde-19feec868159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from ekfplot import plot as ek, colors as ec\n",
    "from ekfstats import math, fit, imstats\n",
    "\n",
    "from pieridae.starbursts import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7018b26b-9a4d-44e4-9f48-4ab5c576abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1547ecf-1998-4296-a938-eac372eb8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ekfstats import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267eaa2a-c278-4cab-afa2-19e932135e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('../local_data/pieridae_output/starlet/starbursts_v0/M*/*i_results.pkl')\n",
    "\n",
    "imgs = []\n",
    "img_names = []\n",
    "for fname in filenames:\n",
    "    img = []\n",
    "    for band in 'gi':\n",
    "        current_filename = fname.replace('_i_',f'_{band}_')\n",
    "\n",
    "        \n",
    "        with open(current_filename,'rb') as f:\n",
    "            xf = pickle.load(f)\n",
    "\n",
    "            img.append(xf['image'])\n",
    "            if band == 'i':\n",
    "                img.append(xf['hf_image'])\n",
    "\n",
    "    \n",
    "    imgs.append(np.array(img))\n",
    "    img_names.append(fname.split('/')[-2])\n",
    "imgs = np.array(imgs)\n",
    "img_names = np.array(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c19fbd-8f27-4343-942b-5a228b0d9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_unlabelled_images():\n",
    "    indices = np.random.permutation(len(imgs))\n",
    "    return torch.tensor(imgs[indices], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0ba48b-b15c-46fd-9eea-1859a283660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kadofong/.conda/envs/merenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kadofong/.conda/envs/merenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Using torchvision transforms (wrapped in nn.Sequential)\n",
    "transform1 = nn.Sequential(\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    ")\n",
    "\n",
    "transform2 = nn.Sequential(\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3)\n",
    ")\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "learner = BYOL(\n",
    "    resnet,\n",
    "    image_size=150,\n",
    "    hidden_layer='avgpool',\n",
    "    projection_size=256,        # Final projection dimension\n",
    "    projection_hidden_size=1024, # Hidden layer in projector MLP\n",
    "    moving_average_decay=0.99,   # Ï„_base for shorter training\n",
    "    use_momentum=True,\n",
    "    augment_fn=transform1,\n",
    "    augment_fn2=transform2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fc5fe-baf4-47c6-9f42-7ede072109e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    images = sample_unlabelled_images()\n",
    "    loss = learner(images)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    learner.update_moving_average() # update moving average of target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73136f-28fb-441b-b2e1-9ddc0178327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection, embedding = learner(torch.tensor(imgs, dtype=torch.float32), return_embedding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26904e34-9792-445a-94ae-3d648aefcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import make_lupton_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61cde7-ab63-40e8-94a5-cea90b89d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find similar images using embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for multiple images\n",
    "embeddings = embedding.detach().numpy()\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Find most similar image to the first one\n",
    "#pairs = np.zeros([len(embeddings),2])\n",
    "#for sidx in range(len(embeddings)):    \n",
    "sidx = 301#np.where(img_names=='M3406229848245433130')[0][0]\n",
    "most_similar_idx = similarity_matrix[sidx].argsort()[-2]  # -1 would be itself\n",
    "#pairs[sidx] = [sidx,most_similar_idx]\n",
    "print(f\"Most similar image to image {sidx}: image {most_similar_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1448bc8-48ca-4176-b7e4-1a71db84b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axarr = plt.subplots(1,2,figsize=(10,4))\n",
    "ek.imshow(imgs[sidx,1],ax=axarr[0], q=0.01)\n",
    "ek.imshow(imgs[most_similar_idx,1],ax=axarr[1], q=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0d9ff-999a-4821-bf10-47931118d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916969a2-baf9-47e4-a0f9-925e6bc27c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import umap\n",
    "\n",
    "def create_embeddings_umap(learner, images, n_components=2, \n",
    "                          n_neighbors=15, min_dist=0.1, metric='cosine',\n",
    "                          random_state=42):\n",
    "    \"\"\"\n",
    "    Extract embeddings from BYOL model and reduce dimensionality using UMAP.\n",
    "    \n",
    "    Args:\n",
    "        learner: Trained BYOL model\n",
    "        images: Input images tensor (batch_size, channels, height, width)\n",
    "        n_components: Number of dimensions to reduce to (2 or 3)\n",
    "        n_neighbors: UMAP parameter controlling local vs global structure (5-50)\n",
    "                    Lower values preserve local structure, higher values preserve global structure\n",
    "        min_dist: UMAP parameter controlling how tightly points are packed (0.001-0.5)\n",
    "                 Lower values create tighter clusters\n",
    "        metric: Distance metric ('cosine', 'euclidean', 'manhattan', etc.)\n",
    "               'cosine' often works well for high-dimensional embeddings\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (reducer, embedding_reduced)\n",
    "            - reducer: Fitted UMAP reducer object (can be used to transform new data)\n",
    "            - embedding_reduced: Numpy array of reduced embeddings, shape (n_samples, n_components)\n",
    "    \n",
    "    Example:\n",
    "        >>> reducer, embedding_2d = create_embeddings_umap(learner, imgs_tensor)\n",
    "        >>> print(f\"Reduced embeddings shape: {embedding_2d.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract embeddings from BYOL model\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        _, embeddings = learner(images, return_embedding=True)\n",
    "    \n",
    "    # Convert to numpy and ensure proper dtype\n",
    "    embeddings_np = embeddings.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Handle potential NaN or inf values\n",
    "    embeddings_np = np.nan_to_num(embeddings_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Ensure n_neighbors is valid (must be less than number of samples)\n",
    "    n_neighbors = min(n_neighbors, len(embeddings_np) - 1)\n",
    "    \n",
    "    # Fit UMAP\n",
    "    print(f\"Fitting UMAP with {len(embeddings_np)} samples...\")\n",
    "    print(f\"Original embeddings shape: {embeddings_np.shape}\")\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    embedding_reduced = reducer.fit_transform(embeddings_np)\n",
    "    \n",
    "    print(f\"UMAP embedding shape: {embedding_reduced.shape}\")\n",
    "    \n",
    "    return reducer, embedding_reduced\n",
    "\n",
    "\n",
    "def visualize_embeddings(embedding_reduced, labels=None, ax=None, figsize=(10, 8), \n",
    "                        title=None, point_size=50, alpha=0.7, colormap='tab10',\n",
    "                        save_path=None):\n",
    "    \"\"\"\n",
    "    Create a scatter plot visualization of reduced embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding_reduced: Numpy array of reduced embeddings, shape (n_samples, 2 or 3)\n",
    "        labels: Optional array of labels for coloring points, shape (n_samples,)\n",
    "        figsize: Tuple of figure dimensions (width, height) in inches\n",
    "        title: Custom title for the plot. If None, uses default title\n",
    "        point_size: Size of scatter plot points\n",
    "        alpha: Transparency of points (0.0 to 1.0)\n",
    "        colormap: Matplotlib colormap name for coloring points when labels are provided\n",
    "        save_path: Optional path to save the figure (e.g., 'plot.png', 'plot.pdf')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (fig, ax) matplotlib figure and axis objects\n",
    "    \n",
    "    Example:\n",
    "        >>> fig, ax = visualize_embeddings(embedding_2d, labels=cluster_labels)\n",
    "        >>> plt.show()\n",
    "        \n",
    "        >>> # For 3D visualization\n",
    "        >>> fig, ax = visualize_embeddings(embedding_3d, labels=labels)\n",
    "        >>> plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine if this is 2D or 3D\n",
    "    is_3d = embedding_reduced.shape[1] == 3\n",
    "    \n",
    "    # Create figure\n",
    "    if ax is None:\n",
    "        if is_3d:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    if labels is not None:\n",
    "        # Color by labels if provided\n",
    "        if is_3d:\n",
    "            scatter = ax.scatter(embedding_reduced[:, 0], embedding_reduced[:, 1], embedding_reduced[:, 2],\n",
    "                               c=labels, cmap=colormap, alpha=alpha, s=point_size)\n",
    "        else:\n",
    "            scatter = ax.scatter(embedding_reduced[:, 0], embedding_reduced[:, 1], \n",
    "                               c=labels, cmap=colormap, alpha=alpha, s=point_size)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(scatter, ax=ax, shrink=0.8 if is_3d else 1.0)\n",
    "        \n",
    "        # Default title with labels\n",
    "        default_title = f'{\"3D \" if is_3d else \"\"}UMAP Visualization of BYOL Embeddings (Colored by Labels)'\n",
    "    else:\n",
    "        # Single color if no labels\n",
    "        if is_3d:\n",
    "            ax.scatter(embedding_reduced[:, 0], embedding_reduced[:, 1], embedding_reduced[:, 2],\n",
    "                      alpha=alpha, s=point_size, c='grey')\n",
    "        else:\n",
    "            ax.scatter(embedding_reduced[:, 0], embedding_reduced[:, 1], \n",
    "                      alpha=alpha, s=point_size, c='grey')\n",
    "        \n",
    "        # Default title without labels\n",
    "        default_title = f'{\"3D \" if is_3d else \"\"}UMAP Visualization of BYOL Embeddings'\n",
    "    \n",
    "    # Set title\n",
    "    #ax.set_title(title if title is not None else default_title)\n",
    "    \n",
    "    # Set axis labels\n",
    "    if is_3d:\n",
    "        ax.set_xlabel('UMAP 1')\n",
    "        ax.set_ylabel('UMAP 2')\n",
    "        ax.set_zlabel('UMAP 3')\n",
    "    else:\n",
    "        ax.set_xlabel('UMAP 1')\n",
    "        ax.set_ylabel('UMAP 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Figure saved to: {save_path}\")\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976d84f-b9b6-4124-ab9b-077f5322d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "#labels = np.repeat(np.arange(len(imgs)//20),20)\n",
    "\n",
    "# Extract embeddings and reduce with UMAP\n",
    "reducer, embedding_2d = create_embeddings_umap(\n",
    "    learner=learner,\n",
    "    images=torch.tensor(imgs, dtype=torch.float32),\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.01,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8409a1c-3a2b-40a2-8fd5-4e8dac5ed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers = pd.read_csv('./classifications_kadofong_20250925.csv', index_col=0)\n",
    "# 1 undisturbed\n",
    "# 2 ambiguous\n",
    "# 3 merger\n",
    "# 4 fragmentation\n",
    "# 5 artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b01663-02e0-4fae-a30d-5d5783b9a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mergers.reindex(img_names)\n",
    "labels = labels.replace(np.nan, 0).values.flatten()\n",
    "\n",
    "#cmap = ec.colormap_from_list(['lightgrey','C0','pink','r','tab:green','C4', 'k'], 'discrete')\n",
    "cmap_1 = ec.colormap_from_list(['C0','tab:orange','r','tab:green','C4','k'], 'discrete')\n",
    "\n",
    "names = {1:'undisturbed',2:'ambiguous',3:'merger',4:'fragmentation',5:'artifact'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f462a-5862-4a86-a86c-afd86f3b153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ovlgrid = gridspec.GridSpec(2, 4)\n",
    "ax1 = fig.add_subplot(ovlgrid[:,:2])\n",
    "ax_clusters = [\n",
    "    fig.add_subplot(ovlgrid[0,2]),\n",
    "    fig.add_subplot(ovlgrid[0,3]),\n",
    "    fig.add_subplot(ovlgrid[1,2]),\n",
    "    fig.add_subplot(ovlgrid[1,3]),\n",
    "]\n",
    "\n",
    "# Visualize the results\n",
    "visualize_embeddings(\n",
    "    embedding_reduced=embedding_2d,\n",
    "    #labels=labels,  # optional\n",
    "    figsize=(12, 10),\n",
    "    point_size=10,\n",
    "    alpha=0.1,\n",
    "    ax=ax1,\n",
    "    #colormap=cmap\n",
    "    #save_path='byol_embeddings.png'\n",
    ")\n",
    "ek.scatter(\n",
    "    embedding_2d[labels>0,0],\n",
    "    embedding_2d[labels>0,1],\n",
    "    c=labels[labels>0],\n",
    "    cmap=cmap_1,\n",
    "    vmin=1,\n",
    "    vmax=6,\n",
    "    ax=ax1,\n",
    "    s=6\n",
    ")\n",
    "\n",
    "cc = ['r','b','lime','magenta']\n",
    "\n",
    "xdx = np.random.uniform(embedding_2d.min(axis=0),embedding_2d.max(axis=0))\n",
    "xdx = [4.,6.5]\n",
    "dist = np.sqrt((embedding_2d[:,0]-xdx[0])**2 + (embedding_2d[:,1]-xdx[1])**2)\n",
    "\n",
    "sidx = 1\n",
    "for ix,lbl in enumerate([1,2,3,4]):\n",
    "    \n",
    "    if sidx >= len(imgs[labels==lbl]):\n",
    "        continue\n",
    "        \n",
    "    if lbl in [1,2,3,4]:\n",
    "        ax = ax_clusters[ix]\n",
    "        ek.imshow( imgs[labels==lbl][sidx][2], ax=ax )\n",
    "        ek.text(0.025,0.975, names[lbl], color=cmap_1((lbl-1.)/5.), bordercolor='w', borderwidth=3, ax=ax)\n",
    "        \n",
    "    ax1.scatter(\n",
    "        embedding_2d[labels==lbl][sidx,0],\n",
    "        embedding_2d[labels==lbl][sidx,1],\n",
    "        c=lbl,\n",
    "        cmap=cmap_1,\n",
    "        edgecolor='k',\n",
    "        vmin=1,\n",
    "        vmax=6,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fa2cf-7916-433c-8414-acf7907bd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd38e8-879e-4ccb-96b0-c5b3a59e0e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca21b87-5568-4f16-8f25-00cbe4bfac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df1929-c69b-42ce-a0b9-803d1865ca5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
