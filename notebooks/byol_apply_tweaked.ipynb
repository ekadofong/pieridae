{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfa25a3-cc2c-4716-9fde-19feec868159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from ekfplot import plot as ek, colors as ec\n",
    "from ekfstats import math, fit, imstats\n",
    "\n",
    "from pieridae.starbursts import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7018b26b-9a4d-44e4-9f48-4ab5c576abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1547ecf-1998-4296-a938-eac372eb8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ekfstats import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267eaa2a-c278-4cab-afa2-19e932135e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('../local_data/pieridae_output/starlet/starbursts_v0/M*/*i_results.pkl')\n",
    "\n",
    "imgs = []\n",
    "img_names = []\n",
    "for fname in filenames:\n",
    "    img = []\n",
    "    for band in 'gi':\n",
    "        current_filename = fname.replace('_i_',f'_{band}_')\n",
    "\n",
    "        \n",
    "        with open(current_filename,'rb') as f:\n",
    "            xf = pickle.load(f)\n",
    "\n",
    "            img.append(xf['image'])\n",
    "            if band == 'i':\n",
    "                img.append(xf['hf_image'])\n",
    "\n",
    "    \n",
    "    imgs.append(np.array(img))\n",
    "    img_names.append(fname.split('/')[-2])\n",
    "imgs = np.array(imgs)\n",
    "img_names = np.array(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c19fbd-8f27-4343-942b-5a228b0d9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_unlabelled_images():\n",
    "    indices = np.random.permutation(len(imgs))\n",
    "    return torch.tensor(imgs[indices], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0ba48b-b15c-46fd-9eea-1859a283660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kadofong/.conda/envs/merenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kadofong/.conda/envs/merenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Using torchvision transforms (wrapped in nn.Sequential)\n",
    "transform1 = nn.Sequential(\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    ")\n",
    "\n",
    "transform2 = nn.Sequential(\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3)\n",
    ")\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "learner = BYOL(\n",
    "    resnet,\n",
    "    image_size=150,\n",
    "    hidden_layer='avgpool',\n",
    "    projection_size=256,        # Final projection dimension\n",
    "    projection_hidden_size=1024, # Hidden layer in projector MLP\n",
    "    moving_average_decay=0.99,   # τ_base for shorter training\n",
    "    use_momentum=True,\n",
    "    augment_fn=transform1,\n",
    "    augment_fn2=transform2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fc5fe-baf4-47c6-9f42-7ede072109e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [09:44<20:09, 35.57s/it]"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "for _ in tqdm(range(50)):\n",
    "    images = sample_unlabelled_images()\n",
    "    loss = learner(images)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    learner.update_moving_average() # update moving average of target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73136f-28fb-441b-b2e1-9ddc0178327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection, embedding = learner(torch.tensor(imgs, dtype=torch.float32), return_embedding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26904e34-9792-445a-94ae-3d648aefcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import make_lupton_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61cde7-ab63-40e8-94a5-cea90b89d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find similar images using embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get embeddings for multiple images\n",
    "embeddings = embedding.detach().numpy()\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Find most similar image to the first one\n",
    "#pairs = np.zeros([len(embeddings),2])\n",
    "#for sidx in range(len(embeddings)):    \n",
    "sidx = 448#np.where(img_names=='M3406229848245433130')[0][0]\n",
    "most_similar_idx = similarity_matrix[sidx].argsort()[-2]  # -1 would be itself\n",
    "#pairs[sidx] = [sidx,most_similar_idx]\n",
    "print(f\"Most similar image to image {sidx}: image {most_similar_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1448bc8-48ca-4176-b7e4-1a71db84b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axarr = plt.subplots(2,2,figsize=(10,8))\n",
    "\n",
    "ek.imshow(imgs[sidx,0],ax=axarr[0,0],  q=0.05)\n",
    "ek.imshow(imgs[most_similar_idx,0],ax=axarr[0,1], q=0.05)\n",
    "ek.imshow(imgs[sidx,2],ax=axarr[1,0],  q=0.05)\n",
    "ek.imshow(imgs[most_similar_idx,2],ax=axarr[1,1], q=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0d9ff-999a-4821-bf10-47931118d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8409a1c-3a2b-40a2-8fd5-4e8dac5ed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers = pd.read_csv('./classifications_kadofong_20250925.csv', index_col=0)\n",
    "# 1 undisturbed\n",
    "# 2 ambiguous\n",
    "# 3 merger\n",
    "# 4 fragmentation\n",
    "# 5 artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b01663-02e0-4fae-a30d-5d5783b9a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mergers.reindex(img_names)\n",
    "labels = labels.replace(np.nan, 0).values.flatten()\n",
    "\n",
    "#cmap = ec.colormap_from_list(['lightgrey','C0','pink','r','tab:green','C4', 'k'], 'discrete')\n",
    "cmap_1 = ec.colormap_from_list(['lightgrey','C0','tab:orange','r','tab:green','C4','k'], 'discrete')\n",
    "\n",
    "names = {1:'undisturbed',2:'ambiguous',3:'merger',4:'fragmentation',5:'artifact'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916969a2-baf9-47e4-a0f9-925e6bc27c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_embeddings_pca_umap(learner, images, \n",
    "                              # PCA parameters\n",
    "                              pca_components=None, explained_variance_threshold=0.95,\n",
    "                              standardize=True, \n",
    "                              # UMAP parameters\n",
    "                              n_components=2, n_neighbors=15, min_dist=0.1, \n",
    "                              metric='euclidean', random_state=42):\n",
    "    \"\"\"\n",
    "    Extract embeddings from BYOL model, apply PCA for dimensionality reduction,\n",
    "    then apply UMAP for visualization and clustering.\n",
    "    \n",
    "    Args:\n",
    "        learner: Trained BYOL model\n",
    "        images: Input images tensor (batch_size, channels, height, width)\n",
    "        \n",
    "        # PCA parameters\n",
    "        pca_components: Number of PCA components to keep. If None, uses explained_variance_threshold\n",
    "        explained_variance_threshold: Keep components that explain this much variance (0.0-1.0)\n",
    "        standardize: Whether to standardize features before PCA\n",
    "        \n",
    "        # UMAP parameters  \n",
    "        n_components: Final number of dimensions (2 or 3 for visualization)\n",
    "        n_neighbors: UMAP parameter controlling local vs global structure (5-50)\n",
    "        min_dist: UMAP parameter controlling cluster tightness (0.001-0.5)\n",
    "        metric: Distance metric for UMAP ('euclidean', 'cosine', 'manhattan')\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains all fitted models and embeddings\n",
    "            - 'scaler': Fitted StandardScaler (if used)\n",
    "            - 'pca': Fitted PCA model\n",
    "            - 'umap': Fitted UMAP model\n",
    "            - 'embeddings_original': Original BYOL embeddings\n",
    "            - 'embeddings_pca': PCA-reduced embeddings\n",
    "            - 'embeddings_umap': Final UMAP embeddings\n",
    "            - 'pca_info': Dictionary with PCA analysis information\n",
    "    \n",
    "    Example:\n",
    "        >>> result = create_embeddings_pca_umap(learner, imgs_tensor, \n",
    "                                               pca_components=50, n_components=2)\n",
    "        >>> print(f\"Original shape: {result['embeddings_original'].shape}\")\n",
    "        >>> print(f\"PCA shape: {result['embeddings_pca'].shape}\")  \n",
    "        >>> print(f\"UMAP shape: {result['embeddings_umap'].shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract embeddings from BYOL model\n",
    "    print(\"Extracting BYOL embeddings...\")\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        _, embeddings = learner(images, return_embedding=True)\n",
    "    \n",
    "    # Convert to numpy and clean data\n",
    "    embeddings_np = embeddings.cpu().numpy().astype(np.float32)\n",
    "    embeddings_np = np.nan_to_num(embeddings_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    print(f\"Original BYOL embeddings shape: {embeddings_np.shape}\")\n",
    "    \n",
    "    # Step 1: Standardization (optional but recommended)\n",
    "    scaler = None\n",
    "    if standardize:\n",
    "        print(\"Standardizing features...\")\n",
    "        scaler = StandardScaler()\n",
    "        embeddings_standardized = scaler.fit_transform(embeddings_np)\n",
    "    else:\n",
    "        embeddings_standardized = embeddings_np\n",
    "    \n",
    "    # Step 2: PCA Analysis and Reduction\n",
    "    print(\"Applying PCA...\")\n",
    "    \n",
    "    # Determine number of PCA components\n",
    "    if pca_components is None:\n",
    "        # Use explained variance threshold to determine components\n",
    "        pca_full = PCA()\n",
    "        pca_full.fit(embeddings_standardized)\n",
    "        cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "        pca_components = np.argmax(cumsum_var >= explained_variance_threshold) + 1\n",
    "        print(f\"Using {pca_components} PCA components to explain {explained_variance_threshold:.1%} variance\")\n",
    "    \n",
    "    # Ensure we don't use more components than available\n",
    "    max_components = min(embeddings_standardized.shape[0] - 1, embeddings_standardized.shape[1])\n",
    "    pca_components = min(pca_components, max_components)\n",
    "    \n",
    "    # Fit PCA with determined number of components\n",
    "    pca = PCA(n_components=pca_components, random_state=random_state)\n",
    "    embeddings_pca = pca.fit_transform(embeddings_standardized)\n",
    "    \n",
    "    # PCA analysis information\n",
    "    pca_info = {\n",
    "        'n_components': pca_components,\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "        'cumulative_explained_variance': np.cumsum(pca.explained_variance_ratio_),\n",
    "        'total_explained_variance': np.sum(pca.explained_variance_ratio_),\n",
    "        'singular_values': pca.singular_values_\n",
    "    }\n",
    "    \n",
    "    print(f\"PCA reduced embeddings shape: {embeddings_pca.shape}\")\n",
    "    print(f\"Total explained variance: {pca_info['total_explained_variance']:.3f}\")\n",
    "    \n",
    "    # Step 3: UMAP on PCA-reduced embeddings\n",
    "    print(\"Applying UMAP to PCA features...\")\n",
    "    \n",
    "    # Adjust n_neighbors for PCA-reduced data\n",
    "    n_neighbors = min(n_neighbors, len(embeddings_pca) - 1)\n",
    "    \n",
    "    # For PCA-reduced features, euclidean distance is often more appropriate\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    embeddings_umap = umap_reducer.fit_transform(embeddings_pca)\n",
    "    \n",
    "    print(f\"Final UMAP embeddings shape: {embeddings_umap.shape}\")\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'umap': umap_reducer,\n",
    "        'embeddings_original': embeddings_np,\n",
    "        'embeddings_pca': embeddings_pca,\n",
    "        'embeddings_umap': embeddings_umap,\n",
    "        'pca_info': pca_info\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_pca_components(pca_info, n_top_components=10, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Analyze and visualize PCA component information.\n",
    "    \n",
    "    Args:\n",
    "        pca_info: Dictionary containing PCA analysis from create_embeddings_pca_umap\n",
    "        n_top_components: Number of top components to show in detail\n",
    "        figsize: Figure size for plots\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle('PCA Analysis of BYOL Embeddings', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Explained variance ratio\n",
    "    axes[0,0].bar(range(1, min(n_top_components + 1, len(pca_info['explained_variance_ratio']) + 1)), \n",
    "                  pca_info['explained_variance_ratio'][:n_top_components])\n",
    "    axes[0,0].set_xlabel('Principal Component')\n",
    "    axes[0,0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0,0].set_title('Individual Component Variance')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cumulative explained variance\n",
    "    axes[0,1].plot(range(1, len(pca_info['cumulative_explained_variance']) + 1), \n",
    "                   pca_info['cumulative_explained_variance'], 'bo-')\n",
    "    axes[0,1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "    axes[0,1].axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')\n",
    "    axes[0,1].set_xlabel('Number of Components')\n",
    "    axes[0,1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[0,1].set_title('Cumulative Variance Explained')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Plot 3: Scree plot (singular values)\n",
    "    axes[1,0].plot(range(1, min(n_top_components + 1, len(pca_info['singular_values']) + 1)), \n",
    "                   pca_info['singular_values'][:n_top_components], 'go-')\n",
    "    axes[1,0].set_xlabel('Principal Component')\n",
    "    axes[1,0].set_ylabel('Singular Value')\n",
    "    axes[1,0].set_title('Scree Plot')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Component importance summary\n",
    "    top_5_variance = pca_info['explained_variance_ratio'][:5]\n",
    "    axes[1,1].pie(list(top_5_variance) + [1 - sum(top_5_variance)], \n",
    "                  labels=[f'PC{i+1}' for i in range(5)] + ['Others'], \n",
    "                  autopct='%1.1f%%')\n",
    "    axes[1,1].set_title('Top 5 Components Contribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def transform_new_data(result_dict, new_images, learner):\n",
    "    \"\"\"\n",
    "    Transform new images using the fitted PCA and UMAP models.\n",
    "    \n",
    "    Args:\n",
    "        result_dict: Dictionary returned from create_embeddings_pca_umap\n",
    "        new_images: New images to transform\n",
    "        learner: BYOL model (same as used for fitting)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Transformed embeddings at each stage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract new embeddings\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        _, new_embeddings = learner(new_images, return_embedding=True)\n",
    "    \n",
    "    new_embeddings_np = new_embeddings.cpu().numpy().astype(np.float32)\n",
    "    new_embeddings_np = np.nan_to_num(new_embeddings_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Apply same transformations\n",
    "    if result_dict['scaler'] is not None:\n",
    "        new_embeddings_scaled = result_dict['scaler'].transform(new_embeddings_np)\n",
    "    else:\n",
    "        new_embeddings_scaled = new_embeddings_np\n",
    "    \n",
    "    new_embeddings_pca = result_dict['pca'].transform(new_embeddings_scaled)\n",
    "    new_embeddings_umap = result_dict['umap'].transform(new_embeddings_pca)\n",
    "    \n",
    "    return {\n",
    "        'embeddings_original': new_embeddings_np,\n",
    "        'embeddings_pca': new_embeddings_pca,\n",
    "        'embeddings_umap': new_embeddings_umap\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_embeddings_comparison(result_dict, labels=None, figsize=(15, 5), \n",
    "                                  save_path=None):\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison of original vs PCA+UMAP embeddings.\n",
    "    \n",
    "    Args:\n",
    "        result_dict: Dictionary from create_embeddings_pca_umap\n",
    "        labels: Optional labels for coloring points\n",
    "        figsize: Figure size\n",
    "        save_path: Optional save path\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: First 2 PCA components\n",
    "    if labels is not None:\n",
    "        scatter1 = axes[0].scatter(result_dict['embeddings_pca'][:, 0], \n",
    "                                  result_dict['embeddings_pca'][:, 1],                                    \n",
    "                                  c=labels, alpha=1, s=5, cmap=cmap_1)\n",
    "        plt.colorbar(scatter1, ax=axes[0])\n",
    "    else:\n",
    "        axes[0].scatter(result_dict['embeddings_pca'][:, 0], \n",
    "                       result_dict['embeddings_pca'][:, 1], \n",
    "                       alpha=0.7, s=50, c='grey')\n",
    "    \n",
    "    axes[0].set_title('PCA (First 2 Components)')\n",
    "    axes[0].set_xlabel('PC1')\n",
    "    axes[0].set_ylabel('PC2')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: UMAP result\n",
    "    if labels is not None:\n",
    "        scatter2 = axes[1].scatter(result_dict['embeddings_umap'][:, 0], \n",
    "                                  result_dict['embeddings_umap'][:, 1], \n",
    "                                   c=labels, alpha=1, s=5, cmap=cmap_1)      \n",
    "        plt.colorbar(scatter2, ax=axes[1])\n",
    "        scatter2 = axes[1].scatter(result_dict['embeddings_umap'][labels==3, 0], \n",
    "                                  result_dict['embeddings_umap'][labels==3, 1], \n",
    "                                   c='r', alpha=1, s=50, cmap=cmap_1)  \n",
    "        scatter2 = axes[1].scatter(result_dict['embeddings_umap'][labels==2, 0], \n",
    "                                  result_dict['embeddings_umap'][labels==2, 1], \n",
    "                                   c='C1', alpha=1, s=50, cmap=cmap_1)          \n",
    "    else:\n",
    "        axes[1].scatter(result_dict['embeddings_umap'][:, 0], \n",
    "                       result_dict['embeddings_umap'][:, 1], \n",
    "                       alpha=0.7, s=50, c='grey')\n",
    "    \n",
    "    axes[1].set_title('PCA + UMAP')\n",
    "    axes[1].set_xlabel('UMAP 1')\n",
    "    axes[1].set_ylabel('UMAP 2')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: PCA analysis\n",
    "    pca_info = result_dict['pca_info']\n",
    "    axes[2].plot(range(1, len(pca_info['cumulative_explained_variance']) + 1), \n",
    "                pca_info['cumulative_explained_variance'], 'bo-')\n",
    "    axes[2].axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "    axes[2].set_xlabel('Number of Components')\n",
    "    axes[2].set_ylabel('Cumulative Variance')\n",
    "    axes[2].set_title('PCA Variance Explained')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Comparison saved to: {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976d84f-b9b6-4124-ab9b-077f5322d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA + UMAP pipeline\n",
    "result = create_embeddings_pca_umap(\n",
    "    learner=learner, \n",
    "    images=torch.tensor(imgs, dtype=torch.float32),\n",
    "    pca_components=50,        # Or None to auto-determine\n",
    "    n_components=2,          # 2D UMAP\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4292a-986c-4a3b-bc5f-3c01b2ad1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_embeddings_comparison(result, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b4e18-1210-41ba-a07a-49f5a7740528",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fig = analyze_pca_components(result['pca_info'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df1929-c69b-42ce-a0b9-803d1865ca5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
